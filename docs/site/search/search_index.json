{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"regenie regenie is a C++ program for whole genome regression modelling of large genome-wide association studies . It is developed and supported by a team of scientists at the Regeneron Genetics Center. The method has the following properties It works on quantitative and binary traits, including binary traits with unbalanced case-control ratios It can handle population structure and relatedness It can process multiple phenotypes at once efficiently For binary traits, it supports Firth logistic regression and an SPA test It can perform gene/region-based tests (Burden, SKAT/SKATO, ACATV/ACATO) It can perform interaction tests (GxE, GxG) as well as conditional analyses It is fast and memory efficient \ud83d\udd25 It supports the BGEN , PLINK bed/bim/fam and PLINK2 pgen/pvar/psam genetic data formats It is ideally suited for implementation in Apache Spark (see GLOW ) It can be installed with Conda Citation Mbatchou, J., Barnard, L., Backman, J. et al. Computationally efficient whole-genome regression for quantitative and binary traits. Nat Genet 53, 1097\u20131103 (2021). https://doi.org/10.1038/s41588-021-00870-7 License regenie is distributed under an MIT license . Contact If you have any questions about regenie please contact jonathan.marchini@regeneron.com joelle.mbatchou@regeneron.com If you want to submit a issue concerning the software please do so using the regenie Github repository .","title":"Home"},{"location":"#regenie","text":"regenie is a C++ program for whole genome regression modelling of large genome-wide association studies . It is developed and supported by a team of scientists at the Regeneron Genetics Center. The method has the following properties It works on quantitative and binary traits, including binary traits with unbalanced case-control ratios It can handle population structure and relatedness It can process multiple phenotypes at once efficiently For binary traits, it supports Firth logistic regression and an SPA test It can perform gene/region-based tests (Burden, SKAT/SKATO, ACATV/ACATO) It can perform interaction tests (GxE, GxG) as well as conditional analyses It is fast and memory efficient \ud83d\udd25 It supports the BGEN , PLINK bed/bim/fam and PLINK2 pgen/pvar/psam genetic data formats It is ideally suited for implementation in Apache Spark (see GLOW ) It can be installed with Conda","title":"regenie"},{"location":"#citation","text":"Mbatchou, J., Barnard, L., Backman, J. et al. Computationally efficient whole-genome regression for quantitative and binary traits. Nat Genet 53, 1097\u20131103 (2021). https://doi.org/10.1038/s41588-021-00870-7","title":"Citation"},{"location":"#license","text":"regenie is distributed under an MIT license .","title":"License"},{"location":"#contact","text":"If you have any questions about regenie please contact jonathan.marchini@regeneron.com joelle.mbatchou@regeneron.com If you want to submit a issue concerning the software please do so using the regenie Github repository .","title":"Contact"},{"location":"faq/","text":"Frequently asked questions General Why doesn\u2019t regenie need a genetic relatedness matrix (GRM)? regenie performs whole genome regression using the following model Y = X\\beta + \\epsilon where Y_{N\\times 1} is a phenotype, X_{N\\times M} is a genotype matrix, and \\epsilon_i\\sim N(0,\\sigma^2) . This model has close ties to a linear mixed model (LMM) based on an infinitesimal model Y = u + \\epsilon where u\\sim N(0,\\sigma_u^2 K) with K_{N\\times N}=XX^T/M is referred to as the genetic relatedness matrix (GRM). In the LMM, the polygenic effects have been integrated out so that model only involves the GRM $K$ through a variance component in the covariance matrix of the trait. In regenie , we directly estimate the polygenic effects parameter \\beta by using ridge regression, which corresponds to fitting a linear regression model with a L2 penalty to impose shrinkage. Hence, we bypass having to use the GRM K and use the polygenic effect estimates X\\hat{\\beta} to control for population structure when testing variants for association. Can regenie be run on small sample sizes? For quantitative traits, we have not obtained issues running regenie on small data sets. For binary traits, we have obtained successful runs of regenie (step 1 and 2) on data sets with as little as 300 samples. A few factors to consider: Convergence issues may occur in step 1 (all the more if a trait is highly unbalanced) - see below Similarly, convergence issues may occur in step 2 when using Firth approximation - see below Note: we have found that regenie can get conservative in more extreme relatedness scenarios so we recommend not to use it for smaller cohorts with high amounts of relatedness like founder populations where exact mixed-model methods can be used Step 1 What block size to use in step 1? We recommend to use blocks of size 1000 as we have observed that it leads to a reasonable number of ridge predictors at level 1 (e.g. 2,500 with 500K SNPs used and the default regenie parameters) and have noticed little change in the final predictions when varying the block size. How many variants to use in step 1? We recommend to use a smaller set of about 500K directly genotyped SNPs in step 1, which should be sufficient to capture genome-wide polygenic effects. Note that using too many SNPs in Step 1 (e.g. >1M) can lead to a high computational burden due to the resulting higher number of predictors in the level 1 models. What do I do if I get the error \"Uh-oh, SNP XX has low variance (=XX)\" in step 1? This is due to variants with very low minor allele count (MAC) being included in step 1. To avoid this, you should use a MAC filter to remove such variants in a pre-processing step before running Regenie. For example, in PLINK2 you would use the --mac option and obtain a list of variants that pass the MAC filter (note that if you are using --keep/--remove in Regenie, you should also use it in the PLINK2 command) plink2 \\ --bfile my_bed_file \\ --mac 100 \\ --write-snplist \\ --out snps_pass You would then use the output file in regenie as --extract snps_pass.snplist (and this would avoid having to make a new genotype file). What to do if Step 1 of regenie failed for a binary trait when fitting the penalized logsitic regression model? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. If using K-fold CV, switch to LOOCV (option --loocv ) to increase the size of the sample used to fit the model (note: LOOCV is now used by default when the sample size is below 5,000) If it is due to quasi-separation (i.e. Var(Y)=0 occurred in model fitting), either increase the sample size using LOOCV or increase the MAF threshold for variants included in step 1 analysis Step 2 What to do if Step 2 of regenie fails when fitting the null model for the approximate Firth correction? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. We have implemented the same measures as in the logistf function in R to avoid convergence issues, which include the use of a step size threshold when performing a Newton step. We first try fitting the model with a step size threshold that is more liberal (=25) as well as a maximum number of iterations of 1,000 and if convergence fails, we retry the model fit using a more stringent step size threshold (=5) and a higher threshold for the number of iterations (=5,000), which will slow down convergence. The user can also specify a maximum step size threshold using --maxstep-null (use value <5) as well as increase the maximum number of iterations using --maxiter-null (use value >5000). In that case, no retries are perfomed if convergence fails. We recommend to test chromosomes separately (using --chr ) as these parameters may need to be altered when fitting the null model for each chromosome What is reported in A1FREQ when building masks? For the max and comphet rules, the resulting burden masks take on values in [0,2] just like single variants so we compute A1FREQ the same way as done for single variants (i.e. mean(G)/2 where G is a genotype vector). For the sum rule, A1FREQ is computed as the average of the effect allele frequencies across all sites included in the mask.","title":"F.A.Q."},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"faq/#general","text":"Why doesn\u2019t regenie need a genetic relatedness matrix (GRM)? regenie performs whole genome regression using the following model Y = X\\beta + \\epsilon where Y_{N\\times 1} is a phenotype, X_{N\\times M} is a genotype matrix, and \\epsilon_i\\sim N(0,\\sigma^2) . This model has close ties to a linear mixed model (LMM) based on an infinitesimal model Y = u + \\epsilon where u\\sim N(0,\\sigma_u^2 K) with K_{N\\times N}=XX^T/M is referred to as the genetic relatedness matrix (GRM). In the LMM, the polygenic effects have been integrated out so that model only involves the GRM $K$ through a variance component in the covariance matrix of the trait. In regenie , we directly estimate the polygenic effects parameter \\beta by using ridge regression, which corresponds to fitting a linear regression model with a L2 penalty to impose shrinkage. Hence, we bypass having to use the GRM K and use the polygenic effect estimates X\\hat{\\beta} to control for population structure when testing variants for association. Can regenie be run on small sample sizes? For quantitative traits, we have not obtained issues running regenie on small data sets. For binary traits, we have obtained successful runs of regenie (step 1 and 2) on data sets with as little as 300 samples. A few factors to consider: Convergence issues may occur in step 1 (all the more if a trait is highly unbalanced) - see below Similarly, convergence issues may occur in step 2 when using Firth approximation - see below Note: we have found that regenie can get conservative in more extreme relatedness scenarios so we recommend not to use it for smaller cohorts with high amounts of relatedness like founder populations where exact mixed-model methods can be used","title":"General"},{"location":"faq/#step-1","text":"What block size to use in step 1? We recommend to use blocks of size 1000 as we have observed that it leads to a reasonable number of ridge predictors at level 1 (e.g. 2,500 with 500K SNPs used and the default regenie parameters) and have noticed little change in the final predictions when varying the block size. How many variants to use in step 1? We recommend to use a smaller set of about 500K directly genotyped SNPs in step 1, which should be sufficient to capture genome-wide polygenic effects. Note that using too many SNPs in Step 1 (e.g. >1M) can lead to a high computational burden due to the resulting higher number of predictors in the level 1 models. What do I do if I get the error \"Uh-oh, SNP XX has low variance (=XX)\" in step 1? This is due to variants with very low minor allele count (MAC) being included in step 1. To avoid this, you should use a MAC filter to remove such variants in a pre-processing step before running Regenie. For example, in PLINK2 you would use the --mac option and obtain a list of variants that pass the MAC filter (note that if you are using --keep/--remove in Regenie, you should also use it in the PLINK2 command) plink2 \\ --bfile my_bed_file \\ --mac 100 \\ --write-snplist \\ --out snps_pass You would then use the output file in regenie as --extract snps_pass.snplist (and this would avoid having to make a new genotype file). What to do if Step 1 of regenie failed for a binary trait when fitting the penalized logsitic regression model? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. If using K-fold CV, switch to LOOCV (option --loocv ) to increase the size of the sample used to fit the model (note: LOOCV is now used by default when the sample size is below 5,000) If it is due to quasi-separation (i.e. Var(Y)=0 occurred in model fitting), either increase the sample size using LOOCV or increase the MAF threshold for variants included in step 1 analysis","title":"Step 1"},{"location":"faq/#step-2","text":"What to do if Step 2 of regenie fails when fitting the null model for the approximate Firth correction? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. We have implemented the same measures as in the logistf function in R to avoid convergence issues, which include the use of a step size threshold when performing a Newton step. We first try fitting the model with a step size threshold that is more liberal (=25) as well as a maximum number of iterations of 1,000 and if convergence fails, we retry the model fit using a more stringent step size threshold (=5) and a higher threshold for the number of iterations (=5,000), which will slow down convergence. The user can also specify a maximum step size threshold using --maxstep-null (use value <5) as well as increase the maximum number of iterations using --maxiter-null (use value >5000). In that case, no retries are perfomed if convergence fails. We recommend to test chromosomes separately (using --chr ) as these parameters may need to be altered when fitting the null model for each chromosome What is reported in A1FREQ when building masks? For the max and comphet rules, the resulting burden masks take on values in [0,2] just like single variants so we compute A1FREQ the same way as done for single variants (i.e. mean(G)/2 where G is a genotype vector). For the sum rule, A1FREQ is computed as the average of the effect allele frequencies across all sites included in the mask.","title":"Step 2"},{"location":"install/","text":"Download The regenie source code is hosted on Github . Installation Note: regenie requires compilation with GCC version >= 5.1 (on Linux) or Clang version >=3.3 (on Mac OSX). It also requires having GFortran library installed. Pre-compiled binaries Pre-compiled binaries are available in the Github repository . These are provided for Linux (including Centos7) and Mac OSX computing environments and are statically linked. For the Linux binaries, users should have GLIBC version >= 2.22 installed. Additionally, they are provided compiled with Intel MKL library which will provide speedups for many of the operations done in regenie . Standard installation regenie requires the BGEN library so you will need to download and install that library. Edit the BGEN_PATH variable in the Makefile to the BGEN library path. On the command line type make while in the main source code directory. This should produce the executable called regenie . regenie has been enhanced to allow for gzip compressed input (for phenotype/covariate files) and output (for association results files) using the Boost Iostream library. If this library is installed on the system, you should compile using make HAS_BOOST_IOSTREAM=1 . Furthermore, we have enabled compilation of regenie with the Intel Math Kernel (MKL) library. You first need to have it installed on your system and modify the MKLROOT variable in the Makefile to the installed MKL library path. With CMake You can compile the binary using CMake version >=3.13 (instead of make as above). mkdir -p build cd build BGEN_PATH=<path_to_bgen_lib> cmake .. make This will generate the binary in the build/ subdirectory. To use with Boost Iostreams and/or Intel MKL library, add the corresponding flags before the cmake command on line 3 (e.g. BGEN_PATH=<path_to_bgen_lib> HAS_BOOST_IOSTREAM=1 cmake .. ). With Docker Alternatively, you can use a Docker image to run regenie . A guide to using docker is available on the Github page . With conda To install with conda , you can use the following commands: # create new environment conda create -n regenie_env -c conda-forge -c bioconda regenie # load it conda activate regenie_env Computing requirements We have tested regenie on 64-bit Linux and 64-bit Mac OSX computing environments. Note that for Mac OSX computing environments, compiling is done without OpenMP, as the library is not built-in by default and has to be installed separately. Memory usage In both Step 1 and Step 2 of a regenie run the genetic data file is read once, in blocks of SNPs, so at no point is the full dataset ever stored in memory. regenie uses a dimension reduction approach using ridge regression to produce a relatively small set of genetic predictors, that are then used to fit a whole-genome regression model. These genetic predictors are stored in memory by default, and can be relatively large if many phenotypes are stored at once. For example, if there are P phenotypes, M SNPs and N samples, and a block size of B SNPs is used with R ridge parameters, then regenie needs to store roughly N\\times M/B\\times R doubles per phenotype, which is 8Gb per phenotype when M=500,000, N=400,000, B =1,000,R=5 and 200Gb in total when P=25 . However, the --lowmem option can be used to avoid that memory usage, at negligible extra computational cost, by writing temporary files to disk. Threading regenie can take advantage of multiple cores using threading. The number of threads can be specified using the --threads option. regenie uses the Eigen library for efficient linear algebra operations and this uses threading where possible. For PLINK bed/bim/fam files, PLINK2 pgen/pvar/psam files, as well as BGEN v1.2 files with 8-bit encoding (format used for UK Biobank 500K imputed data), step 2 of regenie has been optimized by using multithreading through OpenMP . When running the SKAT/ACAT gene-based tests, we recommend to use at most 2 threads and instead parallelize the runs over partitions of the genome (e.g. groups of genes). For Windows platforms If you are on a Windows machine, we recommend to use Windows Subsystem for Linux (WSL) to install a Ubuntu distribution so that you will be able to run REGENIE from a Linux terminal. You can download pre-compiled REGENIE binaries from the Github repository (note that you will need to install the libgomp1 library). Note: from your Windows command prompt, you can run REGENIE using wsl regenie .","title":"Install"},{"location":"install/#download","text":"The regenie source code is hosted on Github .","title":"Download"},{"location":"install/#installation","text":"Note: regenie requires compilation with GCC version >= 5.1 (on Linux) or Clang version >=3.3 (on Mac OSX). It also requires having GFortran library installed.","title":"Installation"},{"location":"install/#pre-compiled-binaries","text":"Pre-compiled binaries are available in the Github repository . These are provided for Linux (including Centos7) and Mac OSX computing environments and are statically linked. For the Linux binaries, users should have GLIBC version >= 2.22 installed. Additionally, they are provided compiled with Intel MKL library which will provide speedups for many of the operations done in regenie .","title":"Pre-compiled binaries"},{"location":"install/#standard-installation","text":"regenie requires the BGEN library so you will need to download and install that library. Edit the BGEN_PATH variable in the Makefile to the BGEN library path. On the command line type make while in the main source code directory. This should produce the executable called regenie . regenie has been enhanced to allow for gzip compressed input (for phenotype/covariate files) and output (for association results files) using the Boost Iostream library. If this library is installed on the system, you should compile using make HAS_BOOST_IOSTREAM=1 . Furthermore, we have enabled compilation of regenie with the Intel Math Kernel (MKL) library. You first need to have it installed on your system and modify the MKLROOT variable in the Makefile to the installed MKL library path.","title":"Standard installation"},{"location":"install/#with-cmake","text":"You can compile the binary using CMake version >=3.13 (instead of make as above). mkdir -p build cd build BGEN_PATH=<path_to_bgen_lib> cmake .. make This will generate the binary in the build/ subdirectory. To use with Boost Iostreams and/or Intel MKL library, add the corresponding flags before the cmake command on line 3 (e.g. BGEN_PATH=<path_to_bgen_lib> HAS_BOOST_IOSTREAM=1 cmake .. ).","title":"With CMake"},{"location":"install/#with-docker","text":"Alternatively, you can use a Docker image to run regenie . A guide to using docker is available on the Github page .","title":"With Docker"},{"location":"install/#with-conda","text":"To install with conda , you can use the following commands: # create new environment conda create -n regenie_env -c conda-forge -c bioconda regenie # load it conda activate regenie_env","title":"With conda"},{"location":"install/#computing-requirements","text":"We have tested regenie on 64-bit Linux and 64-bit Mac OSX computing environments. Note that for Mac OSX computing environments, compiling is done without OpenMP, as the library is not built-in by default and has to be installed separately.","title":"Computing requirements"},{"location":"install/#memory-usage","text":"In both Step 1 and Step 2 of a regenie run the genetic data file is read once, in blocks of SNPs, so at no point is the full dataset ever stored in memory. regenie uses a dimension reduction approach using ridge regression to produce a relatively small set of genetic predictors, that are then used to fit a whole-genome regression model. These genetic predictors are stored in memory by default, and can be relatively large if many phenotypes are stored at once. For example, if there are P phenotypes, M SNPs and N samples, and a block size of B SNPs is used with R ridge parameters, then regenie needs to store roughly N\\times M/B\\times R doubles per phenotype, which is 8Gb per phenotype when M=500,000, N=400,000, B =1,000,R=5 and 200Gb in total when P=25 . However, the --lowmem option can be used to avoid that memory usage, at negligible extra computational cost, by writing temporary files to disk.","title":"Memory usage"},{"location":"install/#threading","text":"regenie can take advantage of multiple cores using threading. The number of threads can be specified using the --threads option. regenie uses the Eigen library for efficient linear algebra operations and this uses threading where possible. For PLINK bed/bim/fam files, PLINK2 pgen/pvar/psam files, as well as BGEN v1.2 files with 8-bit encoding (format used for UK Biobank 500K imputed data), step 2 of regenie has been optimized by using multithreading through OpenMP . When running the SKAT/ACAT gene-based tests, we recommend to use at most 2 threads and instead parallelize the runs over partitions of the genome (e.g. groups of genes).","title":"Threading"},{"location":"install/#for-windows-platforms","text":"If you are on a Windows machine, we recommend to use Windows Subsystem for Linux (WSL) to install a Ubuntu distribution so that you will be able to run REGENIE from a Linux terminal. You can download pre-compiled REGENIE binaries from the Github repository (note that you will need to install the libgomp1 library). Note: from your Windows command prompt, you can run REGENIE using wsl regenie .","title":"For Windows platforms"},{"location":"options/","text":"Getting started To run regenie , use the command ./regenie on the command line, followed by options and flags as needed. To get a full list of options use ./regenie --help The directory examples/ contains some small example files that are useful when getting started. A test run on a set of binary traits can be achieved by the following 2 commands. In Step 1 , the whole genome regression model is fit to the traits, and a set of genomic predictions are produced as output ./regenie \\ --step 1 \\ --bed example/example \\ --exclude example/snplist_rm.txt \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 100 \\ --bt --lowmem \\ --lowmem-prefix tmp_rg \\ --out fit_bin_out In Step 2 , a set of imputed SNPs are tested for association using a Firth logistic regression model ./regenie \\ --step 2 \\ --bgen example/example.bgen \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 200 \\ --bt \\ --firth --approx \\ --pThresh 0.01 \\ --pred fit_bin_out_pred.list \\ --out test_bin_out_firth One of the output files from these two commands is included in example/test_bin_out_firth_Y1.regenie . Basic options Input Option Argument Type Description --bgen, --bed, --pgen FILE Required Input genetic data file. Either BGEN file eg. file.bgen , or bed/bim/fam prefix that assumes file.bed , file.bim , file.fam exist, or pgen/pvar/psam prefix that assumes file.pgen , file.pvar , file.psam exist --sample FILE Optional Sample file corresponding to input BGEN file --ref-first FLAG Optional Specify to use the first allele as the reference allele for BGEN or PLINK bed/bim/fam file input [default is to use the last allele as the reference] --keep FILE Optional Inclusion file that lists individuals to retain in the analysis --remove FILE Optional Exclusion file that lists individuals to remove from the analysis --extract FILE Optional Inclusion file that lists IDs of variants to keep --exclude FILE Optional Exclusion file that lists IDs of variants to remove --extract-or FILE Optional Inclusion file that lists IDs of variants to keep regardless of minimum MAC filter --exclude-or FILE Optional Exclusion file that lists IDs of variants to remove unless MAC is above threshold --phenoFile FILE Required Phenotypes file --phenoCol STRING Optional Use for each phenotype you want to include in the analysis --phenoColList STRING Optional Comma separated list of phenotypes to include in the analysis --phenoExcludeList STRING Optional Comma separated list of phenotypes to ignore from the analysis --covarFile FILE Optional Covariates file --covarCol STRING Optional Use for each covariate you want to include in the analysis --covarColList STRING Optional Comma separated list of covariates to include in the analysis --catCovarList STRING Optional Comma separated list of categorical covariates to include in the analysis --covarExcludeList STRING Optional Comma separated list of covariates to ignore --pred FILE Optional File containing predictions from Step 1 (see Overview). This is required for --step 2 --tpheno-file STRING Optional to use a phenotype file in transposed format (e.g. BED format) --tpheno-indexCol INT Optional index of phenotype name column in transposed phenotype file --tpheno-ignoreCols INT Optional indexes of columns to ignore in transposed phenotype file --iid-only FLAG Optional to specify if header in transposed phenotype file only contains sample IID (assume FID=IID) Note: Parameter expansion can be used when specifying phenotypes/covariates (e.g. --covarCol PC{1:10} ). Also, multiple files can be specified for --extract/--exclude/--keep/--remove by using a comma-separated list. Genetic data file format regenie can read BGEN files, bed/bim/fam files or pgen/psam/pvar files in Step 1 and Step 2. The BGEN file format is described here . The bed/bim/fam file format is described here . The pgen/pvar/psam file format is described here . Tools useful for genetic data file format conversion are : PLINK , QCTOOL , BCFTOOLS . Step 2 of regenie can be sped up by using BGEN files using v1.2 format with 8 bits encoding (genotype file can be generated with PLINK2 using option --export bgen-1.2 'bits=8' ) as well as having an accompanying .bgi index file (a useful tool to create such file is bgenix which is part of the BGEN library). To include X chromosome genotypes in step 1 and/or step 2, males should be coded as diploid so that their genotypes are 0/2 (this is done automatically for BED and PGEN file formats with haploid genotypes). Chromosome values of 23 (for human analyses), X, Y, XY, PAR1 and PAR2 are all acceptable and will be collapsed into a single chromosome. Sample inclusion/exclusion file format 2 2 7 7 . No header. Each line starts with individual FID IID. Space/tab separated. Samples listed in the file that are not in bgen/bed/pgen file are ignored. Variant inclusion/exclusion file format 20 31 . No header. Each line must start with variant ID (if there are additional columns, file must be space/tab separated). Variants listed in this file that are not in bgen/bed/pgen file are ignored. Covariate file format FID IID V1 V2 V3 1 1 1.46837294454993 1.93779743016325 0.152887004505393 2 2 -1.2234390803815 -1.63408619199948 -0.190201446835255 3 3 0.0711531925667286 0.0863906292357564 0.14254739715665 . Line 1 : Header with FID, IID and C covariate names. Followed by lines of C+2 values. Space/tab separated. Each line contains individual FID and IID followed by C covariate values. Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis as well as samples with missing values at any of the covariates included. If --step 2 is specified, then the covariate file should be the same as that used in Step 1. Phenotype file format FID IID Y1 Y2 1 1 1.64818554321186 2.2765234736685 2 2 -2.67352013711554 -1.53680421614647 3 3 0.217542851471485 0.437289912695016 . Line 1 : Header with FID, IID and P phenotypes names. Followed by lines of P+2 values. Space/tab separated. Each line contains individual FID and IID followed by P phenotype values (for binary traits, must be coded as 0=control, 1=case, NA=missing unless using --1 ). Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. Missing values must be coded as NA. With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait . In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes, use option --strict in Step 1 and 2. If using the transposed phenotype file format with option --tpheno-file , the header line must contain subject IDs as \"FID_IID\", otherwise use option --iid-only and only include IIDs (so will assume FID=IID). Predictions file format Running --step 1 --out foo will produce A set of files containing genomic predictions for each phenotype from Step 1 (see Output section below). A file called foo_pred.list listing the locations of the prediction files. The file list is needed as an input file when using --step 2 via the --pred option. It has one line per phenotype (in any order) that specifies the name of the phenotype and its corresponding prediction file name. Each phenotype must have exactly one prediction file and phenotype names must match with those in the phenotype file. Phenotypes in this file not included in the analysis are ignored. Each prediction file contains the genetic predictions for the phenotype (space separated). Line 1 starts with 'FID_IID' followed by $N$ sample identifiers. It is followed by 23 lines containing the genetic predictions for each chromosome (sex chromosomes are collapsed into chromosome 23). More specifically, each line has $N+1$ values which are the chromosome number followed by the $N$ leave-one chromosome out (LOCO) predictions for each individual. Samples in this file not in the bed/pgen/bgen input file are ignored. Genotyped samples not present in this file will be ignored in the analysis of the corresponding trait. Samples with missing LOCO predictions must have their corresponding phenotype value set to missing. Options Option Argument Type Description --step INT Required specify step for the regenie run (see Overview) [argument can be 1 or 2 ] --qt FLAG Optional specify that traits are quantitative (this is the default so can be ommitted) --bt FLAG Optional specify that traits are binary with 0=control,1=case,NA=missing -1,--cc12 FLAG Optional specify to use 1/2/NA encoding for binary traits (1=control,2=case,NA=missing) --bsize INT Required size of the genotype blocks --cv INT Optional number of cross validation (CV) folds [default is 5] --loocv FLAG Optional flag to use leave-one out cross validation --lowmem FLAG Optional flag to reduce memory usage by writing level 0 predictions to disk (details below). This is very useful if the number of traits is large (e.g. greater than 10) --lowmem-prefix FILE PREFIX Optional prefix where to temporarily write the level 0 predictions --split-l0 PREFIX,N Optional split level 0 across N jobs and set prefix of output files of level 0 predictions --run-l0 FILE,K Optional run level 0 for job K in {1..N} specifying the master file created from '--split-l0' --run-l1 FILE Optional run level 1 specifying the master file from '--split-l0' --l1-phenoList STRING Optional to specify a subset of phenotypes to analyze when using --run-l1 --keep-l0 FLAG Optional avoid deleting the level 0 predictions written on disk after fitting the level 1 models --print-prs FLAG Optional flag to print whole genome predictions (i.e. PRS) without using LOCO scheme --force-step1 FLAG Optional flag to run step 1 when >1M variants are used (not recommened) --minCaseCount INT Optional flag to ignore BTs with low case counts [default is 10] --apply-rint FLAG Optional to apply Rank Inverse Normal Transformation (RINT) to quantitative phenotypes --nb INT Optional number of blocks (determined from block size if not provided) --strict FLAG Optional flag to removing samples with missing data at any of the phenotypes --ignore-pred FLAG Optional skip reading the file specified by --pred (corresponds to simple linear/logistic regression) --use-relative-path FLAG Optional to use relative paths instead of absolute ones for the step 1 output pred.list file --use-prs FLAG Optional flag to use whole genome PRS in --pred (this is output in step 1 when using --print-prs ) --gz FLAG Optional flag to output files in compressed gzip format (LOCO prediction files in step 1 and association results files in step 2) [this only works when compiling with Boost Iostream library (see Install tab)] . --force-impute FLAG Optional flag to keep and impute missing observations for QTs in step 2 --write-samples FLAG Optional flag to write sample IDs for those kept in the analysis for each trait in step 2 --print-pheno FLAG Optional flag to write phenotype name in the first line of the sample ID files when using --write-samples --firth FLAG Optional specify to use Firth likelihood ratio test (LRT) as fallback for p-values less than threshold --approx FLAG Optional flag to use approximate Firth LRT for computational speedup (only works when option --firth is used) --firth-se FLAG Optional flag to compute SE based on effect size and LRT p-value when using Firth correction (instead of based on Hessian of unpenalized log-likelihood) --write-null-firth FLAG Optional to write the null estimates for approximate Firth [can be used in step 1 or 2] --compute-all FLAG Optional to write the null Firth estimates for all chromosomes (regardless of the genotype file) --use-null-firth FILE Optional to use stored null estimates for approximate Firth in step 2 --spa FLAG Optional specify to use Saddlepoint approximation as fallback for p-values less than threshold --pThresh FLOAT Optional P-value threshold below which to apply Firth/SPA correction [default is 0.05] --test STRING Optional specify to carry out dominant or recessive test [default is additive; argument can be dominant or recessive ] --chr INT Optional specify which chromosomes to test in step 2 (use for each chromosome to include) --chrList STRING Optional Comma separated list of chromosomes to test in step 2 --range STRING Optional specify chromosome region for variants to test in step 2 [format=CHR:MINPOS-MAXPOS] --minMAC FLOAT Optional flag to specify the minimum minor allele count (MAC) when testing variants [default is 5]. Variants with lower MAC are ignored. --minINFO FLOAT Optional flag to specify the minimum imputation info score (IMPUTE/MACH R^2) when testing variants. Variants with lower info score are ignored. --sex-specific STRING Optional to perform sex-specific analyses [either 'male'/'female'] --af-cc FLAG Optional to output A1FREQ in case/controls separately in the step 2 result file --no-split FLAG Optional flag to have summary statistics for all traits output in the same file --starting-block INT Optional to start step 2 at a specific block/set number (useful if program crashes during a job) --nauto INT Optional number of autosomal chromosomes (for non-human studies) [default is 22] --maxCatLevels INT Optional maximum number of levels for categorical covariates (for non-human studies) [default is 10] --niter INT Optional maximum number of iterations for logistic regression [default is 30] --maxstep-null INT Optional maximum step size for logistic model with Firth penalty under the null [default is 25] --maxiter-null INT Optional maximum number of iterations for logistic model with Firth penalty under the null [default is 1000] --par-region STRING Optional specify build code to determine bounds for PAR1/PAR2 regions (can be 'b36/b37/b38/hg18/hg19/hg38' or 'start,end' bp bounds of non-PAR region) [default=hg38] --force-qt FLAG Optional force QT run for binary traits --threads INT Optional number of computational threads to use [default=all-1] --debug FLAG Optional debug flag (for use by developers) --verbose FLAG Optional verbose screen output --version FLAG Optional print version number and exit --help FLAG Optional Prints usage and options list to screen When step 1 of regenie is run in low memory mode (i.e. using --lowmem ), temporary files are created on disk (using --lowmem-prefix tmp_prefix determines where the files are written [as in tmp_prefix_l0_Y1 ,..., tmp_prefix_l0_YP for P phenotypes]). If the prefix is not specified, the default is to use the prefix specified by --out (see below). These are automatically deleted at the end of the program (unless the run was not successful in which case the user would need to delete the files) See the Wiki page for more details on how to run the level 0 models for Step 1 of regenie in parallel. Output Option Argument Type Description --out FILE PREFIX Required Output files that depends on --step A log file file.log of the output is generated. Using --step 1 --out file For the P phenotypes, files file_1.loco ,..., file_P.loco are output with the per-chromosome LOCO predictions as rows of the files (following the order of the phenotypes in the phenotype file header). If option --gz was used, the files will be compressed in gzip format and have extension .loco.gz . Genotyped individuals specified using option --remove are excluded from this file. Individuals with missing phenotype values kept in the analysis are included in the file and have their predictions set to missing. The list of blup files needed for step 2 (association testing) is written to file_pred.list . If using --print-prs , files file_1.prs ,..., file_P.prs will be written with the whole genome predictions (i.e. PRS) without using LOCO scheme (similar format as the .loco files). The list of these files is written to file_prs.list and can be used in step 2 with --pred and specifying flag --use-prs . Note that as these are not obtained using a LOCO scheme, association tests could suffer from proximal contamination. If using option --write-null-firth , the estimates for approximate Firth under the null will be written to files file_1.firth,...,file_P.firth and the list of these files is written to file_firth.list . This can be used in step 2 as --use-null-firth file_firth.list . Note that it assumes the same set of covariates are used in Step 1 and 2. Using --step 2 --out file By default, results are written in separate files for each phenotype file_<phenotype1_name>.regenie,...,file_<phenotypeP_name>.regenie . Each file has one line per SNP along with a header line. If option --gz was used, the files will be compressed in gzip format and have extension .regenie.gz . The entries of each row specify chromosome, position, ID, reference allele (allele 0), alternative allele (allele 1), frequency of the alternative allele, sample size and the test performed (additive/dominant/recessive). With BGEN/PGEN files with dosages, the imputation INFO score is provided (IMPUTE info score for BGEN and Mach Rsq for PGEN). Allele frequency, sample size and INFO score, if applicable, are computed using only non-missing samples for each phenotype. These are followed by the estimated effect sizes (for allele 1 on the original scale), standard errors, chi-square test statistics and -\\log_{10} p-value. An additional column is included to specify if Firth/SPA corrections failed. With option --no-split , the summary statistics for all traits are written to a single file file.regenie , with the same format as above. Additionaly, an accompanying file with the trait names corresponding to Y1,Y2,... will be generated in \u2018file.regenie.Ydict\u2019. Note that allele frequency, sample size and INFO score are computed using all analyzed samples. If option --write-samples was used, IDs of samples used for each trait will be written in files file_<phenotype1_name>.regenie.ids,...,file_<phenotypeP_name>.regenie.ids (tab separated, no header). When using --par-region , the default boundaries used for the chrX PAR regions are: b36/hg18: 2709520 and 154584238 b37/hg19: 2699520 and 154931044 b38/hg38: 2781479 and 155701383 Gene-based testing Starting from version 3.0, Step 2 of regenie provides a complimentary set of gene-based test in addition to the burden testing functionality introduced in version 2.0. More specifically, for a given set of variants (eg within a gene) which can be defined using functional annotations, regenie can apply various set-based tests on the variants as well as collapse them into a single combined 'mask' genotype that can be tested for association just like a single variant. Input Option Argument Type Description --anno-file FILE Required File with variant annotations for each set --set-list FILE Required File listing variant sets --extract-sets FILE Optional Inclusion file that lists IDs of variant sets to keep --exclude-sets FILE Optional Exclusion file that lists IDs of variant sets to remove --extract-setlist STRING Optional Comma-separated list of variant sets to keep --exclude-setlist STRING Optional Comma-separated list of variant sets to remove --aaf-file FILE Optional File with variant AAF to use when building masks (instead of AAF estimated from sample) --mask-def FILE Required File with mask definitions using the annotations defined in --anno-file Note: multiple files can be specified for --extract-sets/--exclude-sets by using a comma-separated list. Annotation input files The following files are used to define variant sets and functional annotations which will be used to generate masks. Annotation file 1:55039839:T:C PCSK9 LoF 1:55039842:G:A PCSK9 missense . This file defines functional annotations for variants. It is designed to accommodate for variants with separate annotations for different sets/genes. Each line contains the variant name, the set/gene name and a single annotation category (space/tab separated). Variants not in this file will be assigned to a default \"NULL\" category. A maximum of 63 annotation categories (+NULL category) is allowed. For gene sets, tools you can use to obtain variant annotations per transcripts are snpEFF or VEP . To obtain a single annotation per gene, you could choose the most deleterious functional annotation across the gene transcripts or alternatively use the canonical transcript (note that its definition can vary across software). We have implemented an extended 4-column format of the annotation file which also categorizes sets into domains (e.g. for gene sets, these would correspond to gene domains). 1:55039839:T:C PCSK9 Prodomain LoF 1:55039842:G:A PCSK9 Prodomain missense . Masks will be generated for each domain (maximum of 8 per set/gene) in addition to a mask combining across all domains. Variants can only be assigned to a single domain for each set/gene. Set list file This file lists variants within each set/gene to use when building masks. Each line contains the set/gene name followed by a chromosome and physical position for the set/gene, then by a comma-separated list of variants included in the set/gene. A1BG 19 58346922 19:58346922:C:A,19:58346924:G:A,... A1CF 10 50806630 10:50806630:A:G,10:50806630:A:AT,... . Set inclusion/exclusion file format The file must have a single column of set/gene names corresponding to those in the set list file. PIGP ZBTB38 . AAF file (optional) Both functional annotations and alternative allele frequency (AAF) cutoffs are used when building masks (e.g. only considering LoF sites where AAF is below 1%). By default, the AAF for each variant is computed from the sample but alternatively, the user can specify variant AAFs using this file. Each line contains the variant name followed by its AAF (it should correspond to ALT allele used in the genetic data input). 7:6187101:C:T 1.53918207864341e-05 7:6190395:C:A 2.19920388819247e-06 . Since singleton variants cannot be identified from this file, they are determined by default based on the input genetic data. To enforce which sites should be included in the singleton masks (see --set-singletons ), you can add a third column in the file with a binary indicator (1=singleton; 0=not singleton). So only variants which are specified as singletons will be considered for the singleton masks, regardless of whether they are singletons in the input genetic data. Note that with this flag, singleton sites will be included in all masks (regardless of the AAF in file). 7:6187101:C:T 1.53918207864341e-05 0 7:6190395:C:A 2.19920388819247e-06 1 . Mask definitions Mask file This file specifies which annotation categories should be combined into masks. Each line contains a mask name followed by a comma-seperated list of categories included in the mask (i.e. union is taken over categories). For example below, Mask1 uses only LoF variants and Mask2 uses LoF and missense annotated variants. Mask1 LoF Mask2 LoF,missense . AAF cutoffs Option --aaf-bins specifies the AAF upper bounds used to generate burden masks ( AAF and not MAF [minor allele frequency] is used when deciding which variants go into a mask) . By default, a mask based on singleton sites are always included. For example, --aaf-bins 0.01,0.05 will generate 3 burden masks for AAFs in [0,0.01], [0,0.05] and singletons. SKAT/ACAT tests The option --vc-tests is used to specify the gene-based tests to run. By default, these tests use all variants in each mask category. If you'd like to only include variants whose AAF is below a given threshold ,e.g. only including rare variants, you can use --vc-maxAAF . Test Name in regenie Description SKAT skat Variance component test SKATO skato Omnibus test combining features of SKAT and Burden SKATO-ACAT skato-acat Same as SKATO but using Cauchy combination method to maximize power across SKATO models ACATV acatv Test using Cauchy combination method to combine single-variant p-values ACATO acato Omnibus test combining features of ACATV, SKAT and Burden ACATO-FULL acato-full Same as ACATO but using the larger set of SKATO models used in the SKATO test For example, --vc-tests skato,acato-full will run SKATO and ACATO (both using the default grid of 8 rho values for the SKATO models) and the p-values for SKAT, SKATO, ACATV and ACATO will be output. Ultra-rare variants (defined by default as MAC$\\le$10, see --vc-MACthr ) are collapsed into a burden mask which is then included in the tests instead of the individual variants. For additional details on the tests, see here . Joint test for burden masks The following tests can be used to combine different burden masks generated using different annotation classes as well as AAF thresholds. Test Name in regenie QT BT Robust to LD Assumes same effect direction Minimum P-value minp $\\checkmark$ $\\checkmark$ $\\times$ $\\times$ ACAT acat $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\times$ SBAT sbat $\\checkmark$ $\\times$ $\\checkmark$ $\\checkmark$ The ACAT test combines the p-values of the individual burden masks using the Cauchy combination method (see ref. 14 here ). The SBAT test is described into more detail here . If you only want to output the results for the joint tests (ignore the marginal tests), use --joint-only . LOVO/LODO schemes The leave-one-variant-out (LOVO) scheme takes all sites going into a mask, and builds LOVO masks by leaving out one variant at a time from the full set of sites. The mask including all sites will also be computed. The argument for --mask-lovo is a comma-separated list which consists of the set/gene name, the mask name, and the AAF cutoff (either 'singleton' or a double in (0,1)). If using a 4-column annotation file, then --mask-lovo should have the gene name, the domain name, the mask name, and the AAF cutoff. So the LOVO masks will be generated for a specific gene domain. The leave-one-domain-out (LODO) scheme (specified by --mask-lodo ) takes all sites going into a mask and builds a LODO mask for each domain specified for the gene by excluding all variants in the domain. The full mask including all sites will also be computed. The argument for --mask-lodo should have the gene name, the mask name and the AAF cutoff. Writing mask files Burden masks built in regenie can be written to PLINK bed format. If the input genetic data contains dosages, the masks dosages will be converted to hard-calls prior to being written to file and these hard-calls will be used for the association testing. The PLINK bed file is written using 'ref-last' encoding (i.e. REF allele is listed last in the bim file). Note that this cannot be used with the LOVO/LODO schemes. Options Option Argument Type Description --aaf-bins FLOAT,...,FLOAT Optional comma-separated list of AAF upper bounds to use when building masks [default is a single cutoff of 1%] --build-mask STRING Optional build masks using the maximum number of ALT alleles across sites ( 'max' ; the default), or the sum of ALT alleles ( 'sum' ), or thresholding the sum to 2 ( 'comphet' ) --singleton-carrier FLAG Optional to define singletons as variants with a single carrier in the sample (rather than alternative allele count=1) --set-singletons FLAG Optional to use 3rd column in AAF file to specify variants included in singleton masks --write-mask FLAG Optional write mask to PLINK bed format (does not work when building masks with 'sum') --vc-tests STRING Optional comma-separated list of SKAT/ACAT-type tests to run --vc-maxAAF FLOAT Optional AAF upper bound to use for SKAT/ACAT-type tests [default is 100%] --skat-params FLOAT,FLAT Optional a1,a2 values for the single variant weights computed from Beta(MAF,a1,a2) used in SKAT/ACAT-type tests [default is (1,25)] --skato-rho FLOAT,...,FLOAT Optional comma-separated list of $\\rho$ values used for SKATO models --vc-MACthr FLOAT Optional MAC threshold below which to collapse variants in SKAT/ACAT-type tests [default is 10] --joint STRING Optional comma-separated list of joint tests to apply on the generated burden masks --skip-test FLAG Optional to skip computing association tests after building masks and writing them to file --mask-lovo STRING Optional to perform LOVO scheme --lovo-snplist FILE Optional File with list of variants for which to compute LOVO masks --mask-lodo FLAG Optional to perform LODO scheme --write-mask-snplist FLAG Optional to write list of variants that went into each mask to file --check-burden-files FLAG Optional to check the concordance between annotation, set list and mask files [see below ] --strict-check-burden FLAG Optional to exit early if the annotation, set list and mask definition files dont agree [see below ] Three rules can be used to build masks with --build-mask as shown in diagram below, where the last rule comphet applies a threshold of 2 to the mask from the sum rule. Output With --out file Results are written in separate files for each phenotype file_<phenotype1_name>.regenie,...,file_<phenotypeP_name>.regenie with the same output format mentioned above . Additionally, a header line is included (starting with ## ) which contains mask definition information. Masks will have name <set_name>.<mask_name>.<AAF_cutoff> with the chromosome and physical position having been defined in the set list file, and the reference allele being ref , and the alternate allele corresponding to <mask_name>.<AAF_cutoff> . When using --mask-lovo , the mask name will be the same as above but have suffix _<variant_name> to specify the variant which was excluded when building the mask. With --build-mask sum , the reported mask AAF corresponds to the average AAF across sites included in the mask. If using --write-mask , the masks will be saved to file_masks.{bed,bim,fam} and if using --write-mask-snplist , the list of variants included in each mask will be saved to file_masks.snplist . Example run Using Step 1 results from the Step 1 command above , we use the following command to build and test masks in Step 2 ./regenie \\ --step 2 \\ --bed example/example_3chr \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --bt \\ --remove example/fid_iid_to_remove.txt \\ --firth --approx \\ --pred fit_bin_out_pred.list \\ --anno-file example/example_3chr.annotations \\ --set-list example/example_3chr.setlist \\ --mask-def example/example_3chr.masks \\ --aaf-bins 0.1,0.05 \\ --write-mask \\ --bsize 200 \\ --out test_bin_out_firth For each set, this will produce masks using 3 AAF cutoffs (singletons, 5% and 10% AAF). The masks are written to PLINK bed file (in test_bin_out_firth_masks.{bed,bim,fam} ) and tested for association with each binary trait using Firth approximate test (summary stats in test_bin_out_firth_<phenotype_name>.regenie ). Note that the test uses the whole genome regression LOCO PRS from Step 1 of regenie (specified by --pred ). Checking input files To assess the concordance between the input files for building masks, you can use --check-burden-files which will generate a report in file_masks_report.txt containing: for each set, the list the variants in the set-list file which are unrecognized (not genotyped or not present in annotation file for the set) for each mask, the list of annotations in the mask definition file which are not in the annotation file Additionally, you can use --strict-check-burden to enforce full agreement between the three files (if not, program will terminate) : all genotyped variants in the set list file must be in the annotation file (for the corresponding set) all annotations in the mask definition file must be present in the annotation file Interaction testing Starting from regenie v3.0, you can perform scans for interactions (either GxE or GxG). For GxE tests, the interacting variable should be part of the covariate file (if it is categorical, specify it in --catCovarList ). For GxG tests, the interacting variant can be part of the input genetic file or it can be present in an external file (see --interaction-snp-file ) Options Option Argument Type Description --interaction STRING Optional to run GxE test specifying the interacting covariate (see below) --interaction-snp STRING Optional to run GxG test specifying the interacting variant (see below) --interaction-file FORMAT,FILE Optional external genotype file containing the interacting variant [FORMAT can be bed/bgen/pgen and FILE is the file name (bgen) or file prefix (bed/pgen)] --interaction-file-sample FILE Optional accompagnying sample file for BGEN format --interaction-file-reffirst FLAG Optional use the first allele as the reference for BGEN or PLINK BED formats --no-condtl FLAG Optional to print out all the main effects from the interaction model (see Output section below) --force-condtl FLAG Optional to include the interacting SNP as a covariate in the marginal test (see Output section below) --rare-mac FLOAT Optional minor allele count (MAC) threshold below which to use HLM method for QTs [default is 1000] For GxE tests where the interacting variable is categorical, you can specify the baseline level using --interaction VARNAME[BASE_LEVEL] (e.g. --interaction BMI[<25] ). Otherwise, the first value found in the covariate file will be used as the baseline level. For GxG tests, the default coding for the interacting variant is additive. If you would like to use dominant/recessive/categorical coding, use --interaction-snp SNP_NAME[dom/rec/cat] (for example with dominant coding, --interaction-snp SNPNAME[dom] will allow for separate effects between carriers vs non-carriers of the interacting variant). The allowed values in the brackets are add/dom/rec/cat . Output The result files will contain multiple lines for the same variant corresponding to the different null hypotheses being tested in the interaction model g(\\mu) = E\\alpha + G\\beta + (G\\odot E)\\gamma The suffix in the \"TEST\" column indicates which hypothesis is being tested: \"ADD\": marginal test where the interacting variable has not been added as a covariate $-$ this corresponds to $H_0: \\beta = 0$ given $\\alpha=\\gamma = 0$ this is only printed for GxG tests by default, or GxE using --no-condtl \"ADD-CONDTL\": marginal test where the interacting variable has been added as a covariate (default for GxE tests) $-$ this corresponds to $H_0: \\beta = 0$ given $\\gamma = 0$ this is only printed for GxE tests by default, or GxG using --force-condtl \"ADD-INT_VAR\": test for the main effect of the interaction variable (\"VAR\" will be replaced by the name of the interacting variable) $-$ this corresponds to $H_0: \\alpha = 0$ this is only printed for GxG tests by default, or GxE using --no-condtl If the interacting variable is categorical, you will have separate lines for each level aside from the baseline level (e.g. \"ADD-INT_BMI=25-30\" and \"ADD-INT_BMI=30+\" where baseline level is \"$<$25\") will also output the effect of $E^2$ in \"ADD-INT_VAR^2\" if the trait is binary (see here ) \"ADD-INT_SNP\": test for main effect of tested SNP in the interaction model $-$ this corresponds to $H_0: \\beta = 0$ \"ADD-INT_SNPxVAR\": test for interaction effect (\"VAR\" will be replaced by the name of the interacting variable) $-$ this corresponds to $H_0: \\gamma = 0$ If the interacting variable is categorical, you will have separate lines for each level aside from the baseline level (e.g. \"ADD-INT_SNPxBMI=25-30\" and \"ADD-INT_SNPxBMI=30+\" where baseline level is \"$<$25\") With Firth correction, only the effect sizes for the interaction effect at each level will be reported and the LRT p-value will only be computed for the joint test on the interaction effects \"ADD-INT_$k$DF\": joint test for main and interaction effect of tested variant ($k\\ge2$ for categorical interacting variables) $-$ this corresponds to $H_0: \\beta = \\gamma = 0$ Conditional analyses Starting from regenie v3.0, you can specify genetic variants to add to the set of covariates when performing association testing. This works in both step 1 and 2, and can be used in conjunction with the gene-based tests or the interactiong testing feature. The conditioning variants will automatically be ignored from the analysis. Option Argument Type Description --condition-list FILE Required file with list of variants to condition on --condition-file FORMAT,FILE Optional get conditioning variants from external file (same argument format as --interaction-file ) --condition-file-sample FILE Optional accompagnying sample file for BGEN format --max-condition-vars INT Optional maximum number of conditioning variants [default is 10,000]","title":"Documentation"},{"location":"options/#getting-started","text":"To run regenie , use the command ./regenie on the command line, followed by options and flags as needed. To get a full list of options use ./regenie --help The directory examples/ contains some small example files that are useful when getting started. A test run on a set of binary traits can be achieved by the following 2 commands. In Step 1 , the whole genome regression model is fit to the traits, and a set of genomic predictions are produced as output ./regenie \\ --step 1 \\ --bed example/example \\ --exclude example/snplist_rm.txt \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 100 \\ --bt --lowmem \\ --lowmem-prefix tmp_rg \\ --out fit_bin_out In Step 2 , a set of imputed SNPs are tested for association using a Firth logistic regression model ./regenie \\ --step 2 \\ --bgen example/example.bgen \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 200 \\ --bt \\ --firth --approx \\ --pThresh 0.01 \\ --pred fit_bin_out_pred.list \\ --out test_bin_out_firth One of the output files from these two commands is included in example/test_bin_out_firth_Y1.regenie .","title":"Getting started"},{"location":"options/#basic-options","text":"","title":"Basic options"},{"location":"options/#input","text":"Option Argument Type Description --bgen, --bed, --pgen FILE Required Input genetic data file. Either BGEN file eg. file.bgen , or bed/bim/fam prefix that assumes file.bed , file.bim , file.fam exist, or pgen/pvar/psam prefix that assumes file.pgen , file.pvar , file.psam exist --sample FILE Optional Sample file corresponding to input BGEN file --ref-first FLAG Optional Specify to use the first allele as the reference allele for BGEN or PLINK bed/bim/fam file input [default is to use the last allele as the reference] --keep FILE Optional Inclusion file that lists individuals to retain in the analysis --remove FILE Optional Exclusion file that lists individuals to remove from the analysis --extract FILE Optional Inclusion file that lists IDs of variants to keep --exclude FILE Optional Exclusion file that lists IDs of variants to remove --extract-or FILE Optional Inclusion file that lists IDs of variants to keep regardless of minimum MAC filter --exclude-or FILE Optional Exclusion file that lists IDs of variants to remove unless MAC is above threshold --phenoFile FILE Required Phenotypes file --phenoCol STRING Optional Use for each phenotype you want to include in the analysis --phenoColList STRING Optional Comma separated list of phenotypes to include in the analysis --phenoExcludeList STRING Optional Comma separated list of phenotypes to ignore from the analysis --covarFile FILE Optional Covariates file --covarCol STRING Optional Use for each covariate you want to include in the analysis --covarColList STRING Optional Comma separated list of covariates to include in the analysis --catCovarList STRING Optional Comma separated list of categorical covariates to include in the analysis --covarExcludeList STRING Optional Comma separated list of covariates to ignore --pred FILE Optional File containing predictions from Step 1 (see Overview). This is required for --step 2 --tpheno-file STRING Optional to use a phenotype file in transposed format (e.g. BED format) --tpheno-indexCol INT Optional index of phenotype name column in transposed phenotype file --tpheno-ignoreCols INT Optional indexes of columns to ignore in transposed phenotype file --iid-only FLAG Optional to specify if header in transposed phenotype file only contains sample IID (assume FID=IID) Note: Parameter expansion can be used when specifying phenotypes/covariates (e.g. --covarCol PC{1:10} ). Also, multiple files can be specified for --extract/--exclude/--keep/--remove by using a comma-separated list.","title":"Input"},{"location":"options/#genetic-data-file-format","text":"regenie can read BGEN files, bed/bim/fam files or pgen/psam/pvar files in Step 1 and Step 2. The BGEN file format is described here . The bed/bim/fam file format is described here . The pgen/pvar/psam file format is described here . Tools useful for genetic data file format conversion are : PLINK , QCTOOL , BCFTOOLS . Step 2 of regenie can be sped up by using BGEN files using v1.2 format with 8 bits encoding (genotype file can be generated with PLINK2 using option --export bgen-1.2 'bits=8' ) as well as having an accompanying .bgi index file (a useful tool to create such file is bgenix which is part of the BGEN library). To include X chromosome genotypes in step 1 and/or step 2, males should be coded as diploid so that their genotypes are 0/2 (this is done automatically for BED and PGEN file formats with haploid genotypes). Chromosome values of 23 (for human analyses), X, Y, XY, PAR1 and PAR2 are all acceptable and will be collapsed into a single chromosome.","title":"Genetic data file format"},{"location":"options/#sample-inclusionexclusion-file-format","text":"2 2 7 7 . No header. Each line starts with individual FID IID. Space/tab separated. Samples listed in the file that are not in bgen/bed/pgen file are ignored.","title":"Sample inclusion/exclusion file format"},{"location":"options/#variant-inclusionexclusion-file-format","text":"20 31 . No header. Each line must start with variant ID (if there are additional columns, file must be space/tab separated). Variants listed in this file that are not in bgen/bed/pgen file are ignored.","title":"Variant inclusion/exclusion file format"},{"location":"options/#covariate-file-format","text":"FID IID V1 V2 V3 1 1 1.46837294454993 1.93779743016325 0.152887004505393 2 2 -1.2234390803815 -1.63408619199948 -0.190201446835255 3 3 0.0711531925667286 0.0863906292357564 0.14254739715665 . Line 1 : Header with FID, IID and C covariate names. Followed by lines of C+2 values. Space/tab separated. Each line contains individual FID and IID followed by C covariate values. Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis as well as samples with missing values at any of the covariates included. If --step 2 is specified, then the covariate file should be the same as that used in Step 1.","title":"Covariate file format"},{"location":"options/#phenotype-file-format","text":"FID IID Y1 Y2 1 1 1.64818554321186 2.2765234736685 2 2 -2.67352013711554 -1.53680421614647 3 3 0.217542851471485 0.437289912695016 . Line 1 : Header with FID, IID and P phenotypes names. Followed by lines of P+2 values. Space/tab separated. Each line contains individual FID and IID followed by P phenotype values (for binary traits, must be coded as 0=control, 1=case, NA=missing unless using --1 ). Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. Missing values must be coded as NA. With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait . In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes, use option --strict in Step 1 and 2. If using the transposed phenotype file format with option --tpheno-file , the header line must contain subject IDs as \"FID_IID\", otherwise use option --iid-only and only include IIDs (so will assume FID=IID).","title":"Phenotype file format"},{"location":"options/#predictions-file-format","text":"Running --step 1 --out foo will produce A set of files containing genomic predictions for each phenotype from Step 1 (see Output section below). A file called foo_pred.list listing the locations of the prediction files. The file list is needed as an input file when using --step 2 via the --pred option. It has one line per phenotype (in any order) that specifies the name of the phenotype and its corresponding prediction file name. Each phenotype must have exactly one prediction file and phenotype names must match with those in the phenotype file. Phenotypes in this file not included in the analysis are ignored. Each prediction file contains the genetic predictions for the phenotype (space separated). Line 1 starts with 'FID_IID' followed by $N$ sample identifiers. It is followed by 23 lines containing the genetic predictions for each chromosome (sex chromosomes are collapsed into chromosome 23). More specifically, each line has $N+1$ values which are the chromosome number followed by the $N$ leave-one chromosome out (LOCO) predictions for each individual. Samples in this file not in the bed/pgen/bgen input file are ignored. Genotyped samples not present in this file will be ignored in the analysis of the corresponding trait. Samples with missing LOCO predictions must have their corresponding phenotype value set to missing.","title":"Predictions file format"},{"location":"options/#options","text":"Option Argument Type Description --step INT Required specify step for the regenie run (see Overview) [argument can be 1 or 2 ] --qt FLAG Optional specify that traits are quantitative (this is the default so can be ommitted) --bt FLAG Optional specify that traits are binary with 0=control,1=case,NA=missing -1,--cc12 FLAG Optional specify to use 1/2/NA encoding for binary traits (1=control,2=case,NA=missing) --bsize INT Required size of the genotype blocks --cv INT Optional number of cross validation (CV) folds [default is 5] --loocv FLAG Optional flag to use leave-one out cross validation --lowmem FLAG Optional flag to reduce memory usage by writing level 0 predictions to disk (details below). This is very useful if the number of traits is large (e.g. greater than 10) --lowmem-prefix FILE PREFIX Optional prefix where to temporarily write the level 0 predictions --split-l0 PREFIX,N Optional split level 0 across N jobs and set prefix of output files of level 0 predictions --run-l0 FILE,K Optional run level 0 for job K in {1..N} specifying the master file created from '--split-l0' --run-l1 FILE Optional run level 1 specifying the master file from '--split-l0' --l1-phenoList STRING Optional to specify a subset of phenotypes to analyze when using --run-l1 --keep-l0 FLAG Optional avoid deleting the level 0 predictions written on disk after fitting the level 1 models --print-prs FLAG Optional flag to print whole genome predictions (i.e. PRS) without using LOCO scheme --force-step1 FLAG Optional flag to run step 1 when >1M variants are used (not recommened) --minCaseCount INT Optional flag to ignore BTs with low case counts [default is 10] --apply-rint FLAG Optional to apply Rank Inverse Normal Transformation (RINT) to quantitative phenotypes --nb INT Optional number of blocks (determined from block size if not provided) --strict FLAG Optional flag to removing samples with missing data at any of the phenotypes --ignore-pred FLAG Optional skip reading the file specified by --pred (corresponds to simple linear/logistic regression) --use-relative-path FLAG Optional to use relative paths instead of absolute ones for the step 1 output pred.list file --use-prs FLAG Optional flag to use whole genome PRS in --pred (this is output in step 1 when using --print-prs ) --gz FLAG Optional flag to output files in compressed gzip format (LOCO prediction files in step 1 and association results files in step 2) [this only works when compiling with Boost Iostream library (see Install tab)] . --force-impute FLAG Optional flag to keep and impute missing observations for QTs in step 2 --write-samples FLAG Optional flag to write sample IDs for those kept in the analysis for each trait in step 2 --print-pheno FLAG Optional flag to write phenotype name in the first line of the sample ID files when using --write-samples --firth FLAG Optional specify to use Firth likelihood ratio test (LRT) as fallback for p-values less than threshold --approx FLAG Optional flag to use approximate Firth LRT for computational speedup (only works when option --firth is used) --firth-se FLAG Optional flag to compute SE based on effect size and LRT p-value when using Firth correction (instead of based on Hessian of unpenalized log-likelihood) --write-null-firth FLAG Optional to write the null estimates for approximate Firth [can be used in step 1 or 2] --compute-all FLAG Optional to write the null Firth estimates for all chromosomes (regardless of the genotype file) --use-null-firth FILE Optional to use stored null estimates for approximate Firth in step 2 --spa FLAG Optional specify to use Saddlepoint approximation as fallback for p-values less than threshold --pThresh FLOAT Optional P-value threshold below which to apply Firth/SPA correction [default is 0.05] --test STRING Optional specify to carry out dominant or recessive test [default is additive; argument can be dominant or recessive ] --chr INT Optional specify which chromosomes to test in step 2 (use for each chromosome to include) --chrList STRING Optional Comma separated list of chromosomes to test in step 2 --range STRING Optional specify chromosome region for variants to test in step 2 [format=CHR:MINPOS-MAXPOS] --minMAC FLOAT Optional flag to specify the minimum minor allele count (MAC) when testing variants [default is 5]. Variants with lower MAC are ignored. --minINFO FLOAT Optional flag to specify the minimum imputation info score (IMPUTE/MACH R^2) when testing variants. Variants with lower info score are ignored. --sex-specific STRING Optional to perform sex-specific analyses [either 'male'/'female'] --af-cc FLAG Optional to output A1FREQ in case/controls separately in the step 2 result file --no-split FLAG Optional flag to have summary statistics for all traits output in the same file --starting-block INT Optional to start step 2 at a specific block/set number (useful if program crashes during a job) --nauto INT Optional number of autosomal chromosomes (for non-human studies) [default is 22] --maxCatLevels INT Optional maximum number of levels for categorical covariates (for non-human studies) [default is 10] --niter INT Optional maximum number of iterations for logistic regression [default is 30] --maxstep-null INT Optional maximum step size for logistic model with Firth penalty under the null [default is 25] --maxiter-null INT Optional maximum number of iterations for logistic model with Firth penalty under the null [default is 1000] --par-region STRING Optional specify build code to determine bounds for PAR1/PAR2 regions (can be 'b36/b37/b38/hg18/hg19/hg38' or 'start,end' bp bounds of non-PAR region) [default=hg38] --force-qt FLAG Optional force QT run for binary traits --threads INT Optional number of computational threads to use [default=all-1] --debug FLAG Optional debug flag (for use by developers) --verbose FLAG Optional verbose screen output --version FLAG Optional print version number and exit --help FLAG Optional Prints usage and options list to screen When step 1 of regenie is run in low memory mode (i.e. using --lowmem ), temporary files are created on disk (using --lowmem-prefix tmp_prefix determines where the files are written [as in tmp_prefix_l0_Y1 ,..., tmp_prefix_l0_YP for P phenotypes]). If the prefix is not specified, the default is to use the prefix specified by --out (see below). These are automatically deleted at the end of the program (unless the run was not successful in which case the user would need to delete the files) See the Wiki page for more details on how to run the level 0 models for Step 1 of regenie in parallel.","title":"Options"},{"location":"options/#output","text":"Option Argument Type Description --out FILE PREFIX Required Output files that depends on --step A log file file.log of the output is generated. Using --step 1 --out file For the P phenotypes, files file_1.loco ,..., file_P.loco are output with the per-chromosome LOCO predictions as rows of the files (following the order of the phenotypes in the phenotype file header). If option --gz was used, the files will be compressed in gzip format and have extension .loco.gz . Genotyped individuals specified using option --remove are excluded from this file. Individuals with missing phenotype values kept in the analysis are included in the file and have their predictions set to missing. The list of blup files needed for step 2 (association testing) is written to file_pred.list . If using --print-prs , files file_1.prs ,..., file_P.prs will be written with the whole genome predictions (i.e. PRS) without using LOCO scheme (similar format as the .loco files). The list of these files is written to file_prs.list and can be used in step 2 with --pred and specifying flag --use-prs . Note that as these are not obtained using a LOCO scheme, association tests could suffer from proximal contamination. If using option --write-null-firth , the estimates for approximate Firth under the null will be written to files file_1.firth,...,file_P.firth and the list of these files is written to file_firth.list . This can be used in step 2 as --use-null-firth file_firth.list . Note that it assumes the same set of covariates are used in Step 1 and 2. Using --step 2 --out file By default, results are written in separate files for each phenotype file_<phenotype1_name>.regenie,...,file_<phenotypeP_name>.regenie . Each file has one line per SNP along with a header line. If option --gz was used, the files will be compressed in gzip format and have extension .regenie.gz . The entries of each row specify chromosome, position, ID, reference allele (allele 0), alternative allele (allele 1), frequency of the alternative allele, sample size and the test performed (additive/dominant/recessive). With BGEN/PGEN files with dosages, the imputation INFO score is provided (IMPUTE info score for BGEN and Mach Rsq for PGEN). Allele frequency, sample size and INFO score, if applicable, are computed using only non-missing samples for each phenotype. These are followed by the estimated effect sizes (for allele 1 on the original scale), standard errors, chi-square test statistics and -\\log_{10} p-value. An additional column is included to specify if Firth/SPA corrections failed. With option --no-split , the summary statistics for all traits are written to a single file file.regenie , with the same format as above. Additionaly, an accompanying file with the trait names corresponding to Y1,Y2,... will be generated in \u2018file.regenie.Ydict\u2019. Note that allele frequency, sample size and INFO score are computed using all analyzed samples. If option --write-samples was used, IDs of samples used for each trait will be written in files file_<phenotype1_name>.regenie.ids,...,file_<phenotypeP_name>.regenie.ids (tab separated, no header). When using --par-region , the default boundaries used for the chrX PAR regions are: b36/hg18: 2709520 and 154584238 b37/hg19: 2699520 and 154931044 b38/hg38: 2781479 and 155701383","title":"Output"},{"location":"options/#gene-based-testing","text":"Starting from version 3.0, Step 2 of regenie provides a complimentary set of gene-based test in addition to the burden testing functionality introduced in version 2.0. More specifically, for a given set of variants (eg within a gene) which can be defined using functional annotations, regenie can apply various set-based tests on the variants as well as collapse them into a single combined 'mask' genotype that can be tested for association just like a single variant.","title":"Gene-based testing"},{"location":"options/#input_1","text":"Option Argument Type Description --anno-file FILE Required File with variant annotations for each set --set-list FILE Required File listing variant sets --extract-sets FILE Optional Inclusion file that lists IDs of variant sets to keep --exclude-sets FILE Optional Exclusion file that lists IDs of variant sets to remove --extract-setlist STRING Optional Comma-separated list of variant sets to keep --exclude-setlist STRING Optional Comma-separated list of variant sets to remove --aaf-file FILE Optional File with variant AAF to use when building masks (instead of AAF estimated from sample) --mask-def FILE Required File with mask definitions using the annotations defined in --anno-file Note: multiple files can be specified for --extract-sets/--exclude-sets by using a comma-separated list.","title":"Input"},{"location":"options/#annotation-input-files","text":"The following files are used to define variant sets and functional annotations which will be used to generate masks.","title":"Annotation input files"},{"location":"options/#annotation-file","text":"1:55039839:T:C PCSK9 LoF 1:55039842:G:A PCSK9 missense . This file defines functional annotations for variants. It is designed to accommodate for variants with separate annotations for different sets/genes. Each line contains the variant name, the set/gene name and a single annotation category (space/tab separated). Variants not in this file will be assigned to a default \"NULL\" category. A maximum of 63 annotation categories (+NULL category) is allowed. For gene sets, tools you can use to obtain variant annotations per transcripts are snpEFF or VEP . To obtain a single annotation per gene, you could choose the most deleterious functional annotation across the gene transcripts or alternatively use the canonical transcript (note that its definition can vary across software). We have implemented an extended 4-column format of the annotation file which also categorizes sets into domains (e.g. for gene sets, these would correspond to gene domains). 1:55039839:T:C PCSK9 Prodomain LoF 1:55039842:G:A PCSK9 Prodomain missense . Masks will be generated for each domain (maximum of 8 per set/gene) in addition to a mask combining across all domains. Variants can only be assigned to a single domain for each set/gene.","title":"Annotation file"},{"location":"options/#set-list-file","text":"This file lists variants within each set/gene to use when building masks. Each line contains the set/gene name followed by a chromosome and physical position for the set/gene, then by a comma-separated list of variants included in the set/gene. A1BG 19 58346922 19:58346922:C:A,19:58346924:G:A,... A1CF 10 50806630 10:50806630:A:G,10:50806630:A:AT,... .","title":"Set list file"},{"location":"options/#set-inclusionexclusion-file-format","text":"The file must have a single column of set/gene names corresponding to those in the set list file. PIGP ZBTB38 .","title":"Set inclusion/exclusion file format"},{"location":"options/#aaf-file-optional","text":"Both functional annotations and alternative allele frequency (AAF) cutoffs are used when building masks (e.g. only considering LoF sites where AAF is below 1%). By default, the AAF for each variant is computed from the sample but alternatively, the user can specify variant AAFs using this file. Each line contains the variant name followed by its AAF (it should correspond to ALT allele used in the genetic data input). 7:6187101:C:T 1.53918207864341e-05 7:6190395:C:A 2.19920388819247e-06 . Since singleton variants cannot be identified from this file, they are determined by default based on the input genetic data. To enforce which sites should be included in the singleton masks (see --set-singletons ), you can add a third column in the file with a binary indicator (1=singleton; 0=not singleton). So only variants which are specified as singletons will be considered for the singleton masks, regardless of whether they are singletons in the input genetic data. Note that with this flag, singleton sites will be included in all masks (regardless of the AAF in file). 7:6187101:C:T 1.53918207864341e-05 0 7:6190395:C:A 2.19920388819247e-06 1 .","title":"AAF file (optional)"},{"location":"options/#mask-definitions","text":"","title":"Mask definitions"},{"location":"options/#mask-file","text":"This file specifies which annotation categories should be combined into masks. Each line contains a mask name followed by a comma-seperated list of categories included in the mask (i.e. union is taken over categories). For example below, Mask1 uses only LoF variants and Mask2 uses LoF and missense annotated variants. Mask1 LoF Mask2 LoF,missense .","title":"Mask file"},{"location":"options/#aaf-cutoffs","text":"Option --aaf-bins specifies the AAF upper bounds used to generate burden masks ( AAF and not MAF [minor allele frequency] is used when deciding which variants go into a mask) . By default, a mask based on singleton sites are always included. For example, --aaf-bins 0.01,0.05 will generate 3 burden masks for AAFs in [0,0.01], [0,0.05] and singletons.","title":"AAF cutoffs"},{"location":"options/#skatacat-tests","text":"The option --vc-tests is used to specify the gene-based tests to run. By default, these tests use all variants in each mask category. If you'd like to only include variants whose AAF is below a given threshold ,e.g. only including rare variants, you can use --vc-maxAAF . Test Name in regenie Description SKAT skat Variance component test SKATO skato Omnibus test combining features of SKAT and Burden SKATO-ACAT skato-acat Same as SKATO but using Cauchy combination method to maximize power across SKATO models ACATV acatv Test using Cauchy combination method to combine single-variant p-values ACATO acato Omnibus test combining features of ACATV, SKAT and Burden ACATO-FULL acato-full Same as ACATO but using the larger set of SKATO models used in the SKATO test For example, --vc-tests skato,acato-full will run SKATO and ACATO (both using the default grid of 8 rho values for the SKATO models) and the p-values for SKAT, SKATO, ACATV and ACATO will be output. Ultra-rare variants (defined by default as MAC$\\le$10, see --vc-MACthr ) are collapsed into a burden mask which is then included in the tests instead of the individual variants. For additional details on the tests, see here .","title":"SKAT/ACAT tests"},{"location":"options/#joint-test-for-burden-masks","text":"The following tests can be used to combine different burden masks generated using different annotation classes as well as AAF thresholds. Test Name in regenie QT BT Robust to LD Assumes same effect direction Minimum P-value minp $\\checkmark$ $\\checkmark$ $\\times$ $\\times$ ACAT acat $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\times$ SBAT sbat $\\checkmark$ $\\times$ $\\checkmark$ $\\checkmark$ The ACAT test combines the p-values of the individual burden masks using the Cauchy combination method (see ref. 14 here ). The SBAT test is described into more detail here . If you only want to output the results for the joint tests (ignore the marginal tests), use --joint-only .","title":"Joint test for burden masks"},{"location":"options/#lovolodo-schemes","text":"The leave-one-variant-out (LOVO) scheme takes all sites going into a mask, and builds LOVO masks by leaving out one variant at a time from the full set of sites. The mask including all sites will also be computed. The argument for --mask-lovo is a comma-separated list which consists of the set/gene name, the mask name, and the AAF cutoff (either 'singleton' or a double in (0,1)). If using a 4-column annotation file, then --mask-lovo should have the gene name, the domain name, the mask name, and the AAF cutoff. So the LOVO masks will be generated for a specific gene domain. The leave-one-domain-out (LODO) scheme (specified by --mask-lodo ) takes all sites going into a mask and builds a LODO mask for each domain specified for the gene by excluding all variants in the domain. The full mask including all sites will also be computed. The argument for --mask-lodo should have the gene name, the mask name and the AAF cutoff.","title":"LOVO/LODO schemes"},{"location":"options/#writing-mask-files","text":"Burden masks built in regenie can be written to PLINK bed format. If the input genetic data contains dosages, the masks dosages will be converted to hard-calls prior to being written to file and these hard-calls will be used for the association testing. The PLINK bed file is written using 'ref-last' encoding (i.e. REF allele is listed last in the bim file). Note that this cannot be used with the LOVO/LODO schemes.","title":"Writing mask files"},{"location":"options/#options_1","text":"Option Argument Type Description --aaf-bins FLOAT,...,FLOAT Optional comma-separated list of AAF upper bounds to use when building masks [default is a single cutoff of 1%] --build-mask STRING Optional build masks using the maximum number of ALT alleles across sites ( 'max' ; the default), or the sum of ALT alleles ( 'sum' ), or thresholding the sum to 2 ( 'comphet' ) --singleton-carrier FLAG Optional to define singletons as variants with a single carrier in the sample (rather than alternative allele count=1) --set-singletons FLAG Optional to use 3rd column in AAF file to specify variants included in singleton masks --write-mask FLAG Optional write mask to PLINK bed format (does not work when building masks with 'sum') --vc-tests STRING Optional comma-separated list of SKAT/ACAT-type tests to run --vc-maxAAF FLOAT Optional AAF upper bound to use for SKAT/ACAT-type tests [default is 100%] --skat-params FLOAT,FLAT Optional a1,a2 values for the single variant weights computed from Beta(MAF,a1,a2) used in SKAT/ACAT-type tests [default is (1,25)] --skato-rho FLOAT,...,FLOAT Optional comma-separated list of $\\rho$ values used for SKATO models --vc-MACthr FLOAT Optional MAC threshold below which to collapse variants in SKAT/ACAT-type tests [default is 10] --joint STRING Optional comma-separated list of joint tests to apply on the generated burden masks --skip-test FLAG Optional to skip computing association tests after building masks and writing them to file --mask-lovo STRING Optional to perform LOVO scheme --lovo-snplist FILE Optional File with list of variants for which to compute LOVO masks --mask-lodo FLAG Optional to perform LODO scheme --write-mask-snplist FLAG Optional to write list of variants that went into each mask to file --check-burden-files FLAG Optional to check the concordance between annotation, set list and mask files [see below ] --strict-check-burden FLAG Optional to exit early if the annotation, set list and mask definition files dont agree [see below ] Three rules can be used to build masks with --build-mask as shown in diagram below, where the last rule comphet applies a threshold of 2 to the mask from the sum rule.","title":"Options"},{"location":"options/#output_1","text":"With --out file Results are written in separate files for each phenotype file_<phenotype1_name>.regenie,...,file_<phenotypeP_name>.regenie with the same output format mentioned above . Additionally, a header line is included (starting with ## ) which contains mask definition information. Masks will have name <set_name>.<mask_name>.<AAF_cutoff> with the chromosome and physical position having been defined in the set list file, and the reference allele being ref , and the alternate allele corresponding to <mask_name>.<AAF_cutoff> . When using --mask-lovo , the mask name will be the same as above but have suffix _<variant_name> to specify the variant which was excluded when building the mask. With --build-mask sum , the reported mask AAF corresponds to the average AAF across sites included in the mask. If using --write-mask , the masks will be saved to file_masks.{bed,bim,fam} and if using --write-mask-snplist , the list of variants included in each mask will be saved to file_masks.snplist .","title":"Output"},{"location":"options/#example-run","text":"Using Step 1 results from the Step 1 command above , we use the following command to build and test masks in Step 2 ./regenie \\ --step 2 \\ --bed example/example_3chr \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --bt \\ --remove example/fid_iid_to_remove.txt \\ --firth --approx \\ --pred fit_bin_out_pred.list \\ --anno-file example/example_3chr.annotations \\ --set-list example/example_3chr.setlist \\ --mask-def example/example_3chr.masks \\ --aaf-bins 0.1,0.05 \\ --write-mask \\ --bsize 200 \\ --out test_bin_out_firth For each set, this will produce masks using 3 AAF cutoffs (singletons, 5% and 10% AAF). The masks are written to PLINK bed file (in test_bin_out_firth_masks.{bed,bim,fam} ) and tested for association with each binary trait using Firth approximate test (summary stats in test_bin_out_firth_<phenotype_name>.regenie ). Note that the test uses the whole genome regression LOCO PRS from Step 1 of regenie (specified by --pred ).","title":"Example run"},{"location":"options/#checking-input-files","text":"To assess the concordance between the input files for building masks, you can use --check-burden-files which will generate a report in file_masks_report.txt containing: for each set, the list the variants in the set-list file which are unrecognized (not genotyped or not present in annotation file for the set) for each mask, the list of annotations in the mask definition file which are not in the annotation file Additionally, you can use --strict-check-burden to enforce full agreement between the three files (if not, program will terminate) : all genotyped variants in the set list file must be in the annotation file (for the corresponding set) all annotations in the mask definition file must be present in the annotation file","title":"Checking input files"},{"location":"options/#interaction-testing","text":"Starting from regenie v3.0, you can perform scans for interactions (either GxE or GxG). For GxE tests, the interacting variable should be part of the covariate file (if it is categorical, specify it in --catCovarList ). For GxG tests, the interacting variant can be part of the input genetic file or it can be present in an external file (see --interaction-snp-file )","title":"Interaction testing"},{"location":"options/#options_2","text":"Option Argument Type Description --interaction STRING Optional to run GxE test specifying the interacting covariate (see below) --interaction-snp STRING Optional to run GxG test specifying the interacting variant (see below) --interaction-file FORMAT,FILE Optional external genotype file containing the interacting variant [FORMAT can be bed/bgen/pgen and FILE is the file name (bgen) or file prefix (bed/pgen)] --interaction-file-sample FILE Optional accompagnying sample file for BGEN format --interaction-file-reffirst FLAG Optional use the first allele as the reference for BGEN or PLINK BED formats --no-condtl FLAG Optional to print out all the main effects from the interaction model (see Output section below) --force-condtl FLAG Optional to include the interacting SNP as a covariate in the marginal test (see Output section below) --rare-mac FLOAT Optional minor allele count (MAC) threshold below which to use HLM method for QTs [default is 1000] For GxE tests where the interacting variable is categorical, you can specify the baseline level using --interaction VARNAME[BASE_LEVEL] (e.g. --interaction BMI[<25] ). Otherwise, the first value found in the covariate file will be used as the baseline level. For GxG tests, the default coding for the interacting variant is additive. If you would like to use dominant/recessive/categorical coding, use --interaction-snp SNP_NAME[dom/rec/cat] (for example with dominant coding, --interaction-snp SNPNAME[dom] will allow for separate effects between carriers vs non-carriers of the interacting variant). The allowed values in the brackets are add/dom/rec/cat .","title":"Options"},{"location":"options/#output_2","text":"The result files will contain multiple lines for the same variant corresponding to the different null hypotheses being tested in the interaction model g(\\mu) = E\\alpha + G\\beta + (G\\odot E)\\gamma The suffix in the \"TEST\" column indicates which hypothesis is being tested: \"ADD\": marginal test where the interacting variable has not been added as a covariate $-$ this corresponds to $H_0: \\beta = 0$ given $\\alpha=\\gamma = 0$ this is only printed for GxG tests by default, or GxE using --no-condtl \"ADD-CONDTL\": marginal test where the interacting variable has been added as a covariate (default for GxE tests) $-$ this corresponds to $H_0: \\beta = 0$ given $\\gamma = 0$ this is only printed for GxE tests by default, or GxG using --force-condtl \"ADD-INT_VAR\": test for the main effect of the interaction variable (\"VAR\" will be replaced by the name of the interacting variable) $-$ this corresponds to $H_0: \\alpha = 0$ this is only printed for GxG tests by default, or GxE using --no-condtl If the interacting variable is categorical, you will have separate lines for each level aside from the baseline level (e.g. \"ADD-INT_BMI=25-30\" and \"ADD-INT_BMI=30+\" where baseline level is \"$<$25\") will also output the effect of $E^2$ in \"ADD-INT_VAR^2\" if the trait is binary (see here ) \"ADD-INT_SNP\": test for main effect of tested SNP in the interaction model $-$ this corresponds to $H_0: \\beta = 0$ \"ADD-INT_SNPxVAR\": test for interaction effect (\"VAR\" will be replaced by the name of the interacting variable) $-$ this corresponds to $H_0: \\gamma = 0$ If the interacting variable is categorical, you will have separate lines for each level aside from the baseline level (e.g. \"ADD-INT_SNPxBMI=25-30\" and \"ADD-INT_SNPxBMI=30+\" where baseline level is \"$<$25\") With Firth correction, only the effect sizes for the interaction effect at each level will be reported and the LRT p-value will only be computed for the joint test on the interaction effects \"ADD-INT_$k$DF\": joint test for main and interaction effect of tested variant ($k\\ge2$ for categorical interacting variables) $-$ this corresponds to $H_0: \\beta = \\gamma = 0$","title":"Output"},{"location":"options/#conditional-analyses","text":"Starting from regenie v3.0, you can specify genetic variants to add to the set of covariates when performing association testing. This works in both step 1 and 2, and can be used in conjunction with the gene-based tests or the interactiong testing feature. The conditioning variants will automatically be ignored from the analysis. Option Argument Type Description --condition-list FILE Required file with list of variants to condition on --condition-file FORMAT,FILE Optional get conditioning variants from external file (same argument format as --interaction-file ) --condition-file-sample FILE Optional accompagnying sample file for BGEN format --max-condition-vars INT Optional maximum number of conditioning variants [default is 10,000]","title":"Conditional analyses"},{"location":"overview/","text":"Overview This page provides an overview of the models and methods implemented in regenie . A full description is given in our paper . regenie carries out genome-wide association tests for both quantitative and binary (case-control) phenotypes. It is designed to handle A large number of samples. For example, it is ideally suited to the UK Biobank dataset with 500,000 samples. A combination of genetic data from a micro-array, imputation and exome sequencing. A large number of either quantitative traits (QTs) or binary (case-control) traits (BTs) Accounting for a set of covariates An overview of the regenie method is provided in the figure below. Essentially, regenie is run in 2 steps: In the first step a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. In the second step, a larger set of genetic markers (e.g. imputed markers) are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1, using a leave one chromosome out (LOCO) scheme, that avoids proximal contamination. Step 1 : Whole genome model In Step 1 a whole genome regression model is fit at a subset of the total set of available genetic markers. These are typically a set of several hundred thousand ( M ) common markers from a micro-array. Ridge regression (level 0) regenie reads in the M markers in blocks of B consecutive markers ( --bsize option). In each block, a set of ridge regression predictors are calculated for a small range of J shrinkage parameters \\{\\tau_1,\\ldots, \\tau_J\\} (using --l0 option [default is 5]) . For a block of SNPs in a N\\times B matrix X and N\\times 1 phenotype vector Y we calculate J predictors X\\widehat{\\beta}_1 \\ldots, X\\widehat{\\beta}_J where \\widehat{\\beta}_j = (X^TX+\\tau_j I)^{-1}X^T Y The idea behind using a range of shrinkage values is to capture the unknown number and size of truly associated genetic markers within each window. The ridge regression takes account of Linkage disequilibrium (LD) within each block. These predictors are stored in place of the genetic markers in matrix W , providing a large reduction in data size. For example, if M=500,000 and B=1,000 and J=5 shrinkage parameters are used, then the reduced dataset will have JM/B=2,500 predictors. Ridge regression is used in this step for both quantitative and binary traits. Cross-validation (level 1) The predictors generated by the ridge regression step will all be positively correlated with the phenotype. Thus, it is important to account for that correlation when building a whole genome wide regression model. When analyzing a quantitative trait we use a second level of ridge regression on the full set of JM/B predictors in W . This approach is inspired by the method of stacked regressions 1 . We fit the ridge regression for a range of shrinkage parameters ( --l1 option) and choose a single best value using K-fold cross validation scheme. This assesses the predictive performance of the model using held out sets of data, and aims to control any over-fitting induced by using the first level of ridge regression to derive the predictors. In other words, we fit the model Y = W\\alpha + \\epsilon where \\alpha is estimated as \\widehat{\\alpha} = (W^TW + \\phi\\,I)^{-1}W^TY and the parameter \\phi is chosen via K-fold cross-validation. For binary traits, we use a logistic ridge regression model to combine the predictors in W \\text{logit}(p) = \\mu + W\\alpha where p is the probability of being a case and \\mu captures the effects of non-genetic covariates. Genetic predictors and LOCO Once \\alpha has been estimated we can construct the genetic prediction Z = W\\widehat{\\alpha} Also, since each column of the matrix W will be associated with a chromosome we can can also construct a genetic prediction ignoring any one chromosome, by simply ignoring those columns when calculating the prediction. This is known as the Leave One Chromosome Out (LOCO) approach. These LOCO predictions are valuable at Step 2 of regenie when each marker is tested for associated (see below). For binary traits, it is the linear predictor in a logistic regression model using LOCO that is saved, and used as an offset when fitting logistic regression models to test for association. Multiple phenotypes The dimension reduction step using ridge regression can be used very efficiently to model multiple phenotypes at once. The ridge regression equations for a block of SNPs in a N\\times B matrix X and a single phenotype in a N\\times 1 matrix Y take the form \\widehat{\\beta} = AY where A = (X^TX+\\tau I)^{-1}X^T does not depend on Y If instead P phenotypes are stored in columns of a N\\times P matrix Y , then the matrix A can be applied jointly to calculate the matrix of estimates \\widehat{\\beta} = AY , and this can take advantage of parallel linear algebra implementations in the Eigen matrix library. Covariates Covariates, such as age and sex and batch effect variables can be included in the regenie model. For quantitative traits, any covariates are regressed out of phenotypes and genotypes before fitting the model. For binary traits, we fit a null model with only covariates, and use predictions from that model as an offset when fitting the logistic regression model. Step 2 : Single-variant association testing In Step 2, a larger set of markers are tested for association with the trait (or traits). As with Step 1, these markers are also read in blocks of B markers, and tested for association. This avoids having to have all markers stored in memory at once. Quantitative traits For quantitative traits, we use a linear regression model for association testing. Covariates are regressed out of the phenotypes and genetic markers. The LOCO predictions from Step 1 are removed from the phenotypes. Linear regression is then used to test association of the residualized phenotype and the genetic marker. Parallel linear algebra operations in the Eigen library are used where possible. Binary traits For binary traits, logistic regression score test is used to test association of the phenotype and the genetic marker. The logistic regression model includes the LOCO predictions from Step 1 as an offset . Covariates are included in the linear predictor in the usual way. When the case-control ratio is imbalanced, standard association tests don't control Type I error well at rare genetic markers. regenie has two options to handle this Firth logistic regression Standard maximum likelihood estimates are generally biased. The Firth correction 2 removes much of the bias, and results in better calibrated test statistics. The correction involves adding a penalty term to the log-likelihood, \\widetilde{l}(\\theta) = l(\\theta) + {1 \\over 2} \\log I|\\theta| where the penalty term corresponds to the use of Jeffrey's Prior. This prior has the effect of shrinking the effect size towards zero. regenie uses a Firth correction when the p-value from the standard logistic regression test is below a threshold (default 0.05). It also includes a novel, accurate and fast approximate Firth correction which is ~60x faster than the exact Firth correction (see the option --firth ). The p-value reported in regenie is based on a likelihood ratio test (LRT), and we use the Hessian of the log-likelihood without the penalty term to estimate the standard error (SE). This may cause an issue in meta-analyses with rare variants, as the effect size estimate and SE may not match with the LRT p-value. Hence, we added an option --firth-se to report a SE computed instead from the effect size estimate and the LRT p-value. Saddle point approxiation (SPA) test The SPA test approximates the null distribution of the test statistic by approximating the cumulant generating function of the test statistic, which involves all of the higher order moments 3 $^,$ 4 . This provides a better estimation of the tail probabilities compared to using standard asymptotic theory which relies on the normal approximation and uses only the first two moments of the dsitribution. A tail probability is obtained as \\begin{align*} P&(T < t_{\\text{obs}}) \\approx \\Phi(z), \\text{ where,}\\\\ z &= w + \\frac{1}{w}\\log{\\frac{v}{w}}\\\\ w &= \\text{sign}(\\delta^*)\\sqrt{ 2 [ t_{\\text{obs}}\\, \\delta^* - K(\\delta^*)}],\\, v = \\delta^*\\sqrt{K''(\\delta^*)} \\end{align*} and K(\\delta) is the cumulant generating function of the test statistic and \\delta^* is obtained by using a root-finding algorithm for K'(\\delta)=t_{\\text{obs}} . As this approximation has been found not to work very well for ultra-rare variants, a minimum minor allele count (MAC) is used to filter out these variants before testing (option --minMAC ). Step 2 : Gene-based testing Instead of performing single-variant association tests, multiple variants can be aggregated in a given region, such as a gene, using the following model g(\\mu) = w_1G_1\\beta_1 + \\dots + w_mG_m\\beta_m where G_i 's represent the single variants included in the test, w_i 's and \\beta_i 's are weights and effect sizes, respectively, for each variant, and g(.) is a link function for the phenotypic mean \\mu . We also denote by S_i the score statistics obtained from the single-variant tests . This can be especially helpful when testing rare variants as single-variant tests usually have lower power performance. To avoid inflation in the gene-based tests due to rare variants as well as reduce computation time, we have implemented the collapsing approach proposed in SAIGE-GENE+ 5 , where ultra-rare variants are aggregated into a mask. For highly imbalanced binary traits, SPA/Firth correction can be used to calibrate the test statistics in the gene-based tests as proposed in Zhao et al. (2020) 6 using --firth/--spa . Burden tests Burden tests, as defined in Lee et al. (2014) 7 , assume \\beta_i=\\beta\\; \\forall i , where \\beta is a fixed coefficient, which then leads to the test statistic Q_{BURDEN} = \\left(\\sum_i w_iS_i\\right)^2 These tests collapse variants into a single variable which is then tested for association with the phenotype. Hence, they are more powerful when variants have effects in the same direction and of similar magnitude. In regenie , multiple options are available to aggregate variants together into a burden mask beyond the linear combination above ( see here ). For example, the burden tests that were employed in Backman et al. (2021) 8 use the default strategy in regenie of collapsing variants by taking the maximum number of rare alleles across the sites. Variance component tests Unlike burden tests, SKAT 9 assume the effect sizes $\\beta_i$ come from an arbitrary distribution with mean 0 and variance $\\tau^2$ which leads to the test statistic Q_{SKAT} = \\sum_i w_i^2S_i^2 Hence, SKAT can remain powerful when variant effects are in opposite directions. The omnibus test SKATO 10 combines the SKAT and burden tests as Q_{SKATO} = \\rho Q_{BURDEN} + (1-\\rho) Q_{SKAT} So setting $\\rho=0$ corresponds to SKAT and $\\rho=1$ to the burden test. In practice, the parameter $\\rho$ is chosen to maximize the power [ regenie uses a default grid of 8 values {$0, 0.1^2, 0.2^2, 0.3^2, 0.4^2, 0.5^2, 0.5, 1$} and set the weights $w_i = Beta(MAF_i,1,25)$]. To obtain the p-value from a linear combination of chi-squared variables, regenie uses Davies' exact method 11 by default. Following Wu et al (2016) 12 , regenie uses Kuonen's saddlepoint approximation method 13 when the Davies' p-value is below 1e-5 and if that fails, it uses Davies' method with more stringent convergence parameters (lim=1e5,acc=1e-9). The original SKATO method uses numerical integration when maximizing power across the various SKATO models that use different values for $\\rho$. We also implement a modification of SKATO, named SKATO-ACAT, which instead uses the Cauchy combination method 14 to combine the p-values from the different SKATO models. Cauchy combination tests The ACATV 15 test uses the Cauchy combination method to combine single variant p-values $p_i$ as Q_{ACATV} = \\sum_i \\widetilde{w}_i^2\\tan{\\{\\pi(0.5 - p_i)\\}} where we set $\\widetilde{w}_i = w_i \\sqrt{MAF(1-MAF)}$. This test is highly computationally tractable and is robust to correlation between the single variant tests. The omnibus test ACATO 15 combines ACATV with the SKAT and burden tests as Q_{ACATO} = \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{ACATV})\\}}+ \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{Burden})\\}}+ \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{SKAT})\\}} where unlike the original ACATO test, we only use one set of the weights $w_i$. Alternatively, we augment the test to include an extended set of SKATO models beyond SKAT and Burden (which correspond to $\\rho$ of 0 and 1 in SKATO respectively) and use the default SKATO grid of 8 values for $\\rho$. Sparse Burden Association Test regenie can generate burden masks which are obtained by aggregating single variants using various annotation classes as well as allele frequency thresholds. The Sparse Burden Association Test (SBAT) 16 combines these burden masks in a joint model imposing constraints of same direction of effects \\mu = \\sum_{\\text{mask }i} M_i\\gamma_i where $M_i$ represent a burden mask and we solve \\underset{\\boldsymbol\\gamma}{\\min} || Y - \\sum_i M_i\\gamma_i||^2 \\text{ subject to } \\gamma_i \\ge 0 \\text{ for all } i The SBAT method tests the hypothesis $H_0: \\gamma_i=0$ for all $i$ vs. $H_1: \\gamma_i > 0$ for some $i$. By using this joint model, the SBAT test accounts for the correlation structure between the burden masks and with the non-negative constraints, it can lead to boost in power performance when multiple burden masks are causal and have concordant effects. This test has the nice property that it combines model selection of the masks (via the sparsity induced by the non-negative assumption) with model inference (it is well calibrated and powerful). Step 2 : Interaction testing The GxE tests are of the form g(\\mu) = E\\alpha + G\\beta + (G\\odot E)\\gamma where $E$ is an environmental risk factor and $G$ is a marker of interest, and $\\odot$ represents the Haddamard (entry-wise) product of the two. The last term in the model allows for the variant to have different effects across values of the risk factor. Note: if $E$ is categorical, we use a dummy variable for each level of $E$ in the model above. We can look at the following hypotheses: $H_0: \\beta = 0$ given $\\gamma = 0$, which is a marginal test for the SNP $H_0: \\beta = 0$, which is a test for the main effect of the SNP in the full model $H_0: \\gamma = 0$, which is a test for interaction $H_0: \\beta = \\gamma = 0$, which tests both main and interaction effects for the SNP Misspecification of the model above, such as in the presence of heteroskedasticity, or the presence of high case-control imbalance can lead to inflation in the tests. Robust (sandwich) standard error (SE) estimators 17 can be used to adress model misspecification however, they can suffer from inflation when testing rare variants or in the presence of high case-control imbalance 18 $^,$ 19 . In regenie , we use a hybrid approach which combines: Wald test with sandwich estimators Wald test with heteroskedastic linear models (for quantitative traits) LRT with penalized Firth regression (for binary traits) For quantitative traits, we use the sandwich estimators HC3 to perform a Wald test for variants whose minor allele count (MAC) is above 1000 (see --rare-mac ). For the remaining variants, we fit a heteroskedastic linear model (HLM) 20 Y = E\\alpha + E^2\\zeta + G\\beta + (G\\odot E)\\gamma + \\epsilon where we assume $\\epsilon \\sim N(0, D)$ where $D$ is a diagonal matrix with entries $\\sigma^2\\exp{(1 + E\\theta_1 + E^2\\theta_2)}$. This formulation allows for the phenotypic variance to also depend on the risk factor $E$. By incorporating both the linear and quadratic effect of $E$ in the mean and variance of $Y$, this model provides robustness to heteroskedasticity ( Note: the $E^2$ terms are only added when $E$ is quantitative ). Wald tests are then performed for the null hypotheses listed above. For binary traits, we consider the following interaction model \\text{logit}(\\mu) = E\\alpha + E^2\\zeta + G\\beta + (G\\odot E)\\gamma where we also include a non-linear effect for $E$ (not if categorical). The sandwich estimator HC3 is used in a Wald test for variants whose MAC is above 1000 (see --rare-mac ) otherwise the model-based standard errors are used. When Firth is specified, we only apply the Firth correction using LRT if the p-value for the interaction term $\\gamma$ from the Wald test is below a specified threshold (see --pThresh ). So the added $E^2$ term as well as the use of the Firth penalty help with case-control imbalance and model misspecification for the effect of $E$ on the phenotype. Missing Phenotype data With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait. In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes from the analysis, use option --strict in step 1 and 2. This can also be used when analyzing a single trait to only keep individuals with complete data by setting the phenotype values of individuals to remove to NA. Note: imputation is only applied to phenotypes; covariates are not allowed to have missing data. References Breiman, L. Stacked regressions. Machine learning 24 , 49--64 (1996). \u21a9 Firth, D. Bias reduction of maximum likelihood estimates. Biometrika 80 , 27--38 (1993). \u21a9 Butler, R. W. Saddlepoint approximations with applications . (Cambridge University Press, 2007). \u21a9 Dey, R., Schmidt, E. M., Abecasis, G. R. & Lee, S. A fast and accurate algorithm to test for binary phenotypes and its application to PheWAS. The American Journal of Human Genetics 101 , 37--49 (2017). \u21a9 Zhou, W. et al. Set-based rare variant association tests for biobank scale sequencing data sets. medRxiv (2021). \u21a9 Zhao, Z. et al. UK biobank whole-exome sequence binary phenome analysis with robust region-based rare-variant test. Am J Hum Genet 106 , 3--12 (2020). \u21a9 Lee, S., Abecasis, G. R., Boehnke, M. & Lin, X. Rare-variant association analysis: Study designs and statistical tests. Am J Hum Genet 95 , 5--23 (2014). \u21a9 Backman, J. D. et al. Exome sequencing and analysis of 454,787 UK biobank participants. Nature 599 , 628--634 (2021). \u21a9 Wu, M. C. et al. Rare-variant association testing for sequencing data with the sequence kernel association test. Am J Hum Genet 89 , 82--93 (2011). \u21a9 Lee, S., Wu, M. C. & Lin, X. Optimal tests for rare variant effects in sequencing association studies. Biostatistics 13 , 762--75 (2012). \u21a9 Davies, R. B. Algorithm AS 155: The distribution of a linear combination of \u03c7 2 random variables. Applied Statistics 29 , 323--333 (1980). \u21a9 Wu, B., Guan, W. & Pankow, J. S. On efficient and accurate calculation of significance p-values for sequence kernel association testing of variant set. Ann Hum Genet 80 , 123--35 (2016). \u21a9 Kuonen, D. Miscellanea. Saddlepoint approximations for distributions of quadratic forms in normal variables. Biometrika 86 , 929--935 (1999). \u21a9 Liu, Y. & Xie, J. Cauchy combination test: A powerful test with analytic p-value calculation under arbitrary dependency structures. J Am Stat Assoc 115 , 393--402 (2020). \u21a9 Liu, Y. et al. ACAT: A fast and powerful p value combination method for rare-variant analysis in sequencing studies. Am J Hum Genet 104 , 410--421 (2019). \u21a9 \u21a9 Ziyatdinov, A., Barber, M. & Marchini, J. Pooling information across burden tests in the UK biobank exome sequencing study. ASHG Conference (2020). \u21a9 MacKinnon, J. G. & White, H. Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. Journal of Econometrics 29 , 305--325 (1985). \u21a9 Tchetgen Tchetgen, E. J. & Kraft, P. On the robustness of tests of genetic associations incorporating gene-environment interaction when the environmental exposure is misspecified. Epidemiology 22 , 257--61 (2011). \u21a9 Voorman, A., Lumley, T., McKnight, B. & Rice, K. Behavior of QQ-plots and genomic control in studies of gene-environment interaction. PLoS One 6 , (2011). \u21a9 Young, A. I., Wauthier, F. L. & Donnelly, P. Identifying loci affecting trait variability and detecting interactions in genome-wide association studies. Nat Genet 50 , 1608--1614 (2018). \u21a9","title":"Overview"},{"location":"overview/#overview","text":"This page provides an overview of the models and methods implemented in regenie . A full description is given in our paper . regenie carries out genome-wide association tests for both quantitative and binary (case-control) phenotypes. It is designed to handle A large number of samples. For example, it is ideally suited to the UK Biobank dataset with 500,000 samples. A combination of genetic data from a micro-array, imputation and exome sequencing. A large number of either quantitative traits (QTs) or binary (case-control) traits (BTs) Accounting for a set of covariates An overview of the regenie method is provided in the figure below. Essentially, regenie is run in 2 steps: In the first step a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. In the second step, a larger set of genetic markers (e.g. imputed markers) are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1, using a leave one chromosome out (LOCO) scheme, that avoids proximal contamination.","title":"Overview"},{"location":"overview/#step-1-whole-genome-model","text":"In Step 1 a whole genome regression model is fit at a subset of the total set of available genetic markers. These are typically a set of several hundred thousand ( M ) common markers from a micro-array.","title":"Step 1 : Whole genome model"},{"location":"overview/#ridge-regression-level-0","text":"regenie reads in the M markers in blocks of B consecutive markers ( --bsize option). In each block, a set of ridge regression predictors are calculated for a small range of J shrinkage parameters \\{\\tau_1,\\ldots, \\tau_J\\} (using --l0 option [default is 5]) . For a block of SNPs in a N\\times B matrix X and N\\times 1 phenotype vector Y we calculate J predictors X\\widehat{\\beta}_1 \\ldots, X\\widehat{\\beta}_J where \\widehat{\\beta}_j = (X^TX+\\tau_j I)^{-1}X^T Y The idea behind using a range of shrinkage values is to capture the unknown number and size of truly associated genetic markers within each window. The ridge regression takes account of Linkage disequilibrium (LD) within each block. These predictors are stored in place of the genetic markers in matrix W , providing a large reduction in data size. For example, if M=500,000 and B=1,000 and J=5 shrinkage parameters are used, then the reduced dataset will have JM/B=2,500 predictors. Ridge regression is used in this step for both quantitative and binary traits.","title":"Ridge regression (level 0)"},{"location":"overview/#cross-validation-level-1","text":"The predictors generated by the ridge regression step will all be positively correlated with the phenotype. Thus, it is important to account for that correlation when building a whole genome wide regression model. When analyzing a quantitative trait we use a second level of ridge regression on the full set of JM/B predictors in W . This approach is inspired by the method of stacked regressions 1 . We fit the ridge regression for a range of shrinkage parameters ( --l1 option) and choose a single best value using K-fold cross validation scheme. This assesses the predictive performance of the model using held out sets of data, and aims to control any over-fitting induced by using the first level of ridge regression to derive the predictors. In other words, we fit the model Y = W\\alpha + \\epsilon where \\alpha is estimated as \\widehat{\\alpha} = (W^TW + \\phi\\,I)^{-1}W^TY and the parameter \\phi is chosen via K-fold cross-validation. For binary traits, we use a logistic ridge regression model to combine the predictors in W \\text{logit}(p) = \\mu + W\\alpha where p is the probability of being a case and \\mu captures the effects of non-genetic covariates.","title":"Cross-validation (level 1)"},{"location":"overview/#genetic-predictors-and-loco","text":"Once \\alpha has been estimated we can construct the genetic prediction Z = W\\widehat{\\alpha} Also, since each column of the matrix W will be associated with a chromosome we can can also construct a genetic prediction ignoring any one chromosome, by simply ignoring those columns when calculating the prediction. This is known as the Leave One Chromosome Out (LOCO) approach. These LOCO predictions are valuable at Step 2 of regenie when each marker is tested for associated (see below). For binary traits, it is the linear predictor in a logistic regression model using LOCO that is saved, and used as an offset when fitting logistic regression models to test for association.","title":"Genetic predictors and LOCO"},{"location":"overview/#multiple-phenotypes","text":"The dimension reduction step using ridge regression can be used very efficiently to model multiple phenotypes at once. The ridge regression equations for a block of SNPs in a N\\times B matrix X and a single phenotype in a N\\times 1 matrix Y take the form \\widehat{\\beta} = AY where A = (X^TX+\\tau I)^{-1}X^T does not depend on Y If instead P phenotypes are stored in columns of a N\\times P matrix Y , then the matrix A can be applied jointly to calculate the matrix of estimates \\widehat{\\beta} = AY , and this can take advantage of parallel linear algebra implementations in the Eigen matrix library.","title":"Multiple phenotypes"},{"location":"overview/#covariates","text":"Covariates, such as age and sex and batch effect variables can be included in the regenie model. For quantitative traits, any covariates are regressed out of phenotypes and genotypes before fitting the model. For binary traits, we fit a null model with only covariates, and use predictions from that model as an offset when fitting the logistic regression model.","title":"Covariates"},{"location":"overview/#step-2-single-variant-association-testing","text":"In Step 2, a larger set of markers are tested for association with the trait (or traits). As with Step 1, these markers are also read in blocks of B markers, and tested for association. This avoids having to have all markers stored in memory at once.","title":"Step 2 : Single-variant association testing"},{"location":"overview/#quantitative-traits","text":"For quantitative traits, we use a linear regression model for association testing. Covariates are regressed out of the phenotypes and genetic markers. The LOCO predictions from Step 1 are removed from the phenotypes. Linear regression is then used to test association of the residualized phenotype and the genetic marker. Parallel linear algebra operations in the Eigen library are used where possible.","title":"Quantitative traits"},{"location":"overview/#binary-traits","text":"For binary traits, logistic regression score test is used to test association of the phenotype and the genetic marker. The logistic regression model includes the LOCO predictions from Step 1 as an offset . Covariates are included in the linear predictor in the usual way. When the case-control ratio is imbalanced, standard association tests don't control Type I error well at rare genetic markers. regenie has two options to handle this","title":"Binary traits"},{"location":"overview/#firth-logistic-regression","text":"Standard maximum likelihood estimates are generally biased. The Firth correction 2 removes much of the bias, and results in better calibrated test statistics. The correction involves adding a penalty term to the log-likelihood, \\widetilde{l}(\\theta) = l(\\theta) + {1 \\over 2} \\log I|\\theta| where the penalty term corresponds to the use of Jeffrey's Prior. This prior has the effect of shrinking the effect size towards zero. regenie uses a Firth correction when the p-value from the standard logistic regression test is below a threshold (default 0.05). It also includes a novel, accurate and fast approximate Firth correction which is ~60x faster than the exact Firth correction (see the option --firth ). The p-value reported in regenie is based on a likelihood ratio test (LRT), and we use the Hessian of the log-likelihood without the penalty term to estimate the standard error (SE). This may cause an issue in meta-analyses with rare variants, as the effect size estimate and SE may not match with the LRT p-value. Hence, we added an option --firth-se to report a SE computed instead from the effect size estimate and the LRT p-value.","title":"Firth logistic regression"},{"location":"overview/#saddle-point-approxiation-spa-test","text":"The SPA test approximates the null distribution of the test statistic by approximating the cumulant generating function of the test statistic, which involves all of the higher order moments 3 $^,$ 4 . This provides a better estimation of the tail probabilities compared to using standard asymptotic theory which relies on the normal approximation and uses only the first two moments of the dsitribution. A tail probability is obtained as \\begin{align*} P&(T < t_{\\text{obs}}) \\approx \\Phi(z), \\text{ where,}\\\\ z &= w + \\frac{1}{w}\\log{\\frac{v}{w}}\\\\ w &= \\text{sign}(\\delta^*)\\sqrt{ 2 [ t_{\\text{obs}}\\, \\delta^* - K(\\delta^*)}],\\, v = \\delta^*\\sqrt{K''(\\delta^*)} \\end{align*} and K(\\delta) is the cumulant generating function of the test statistic and \\delta^* is obtained by using a root-finding algorithm for K'(\\delta)=t_{\\text{obs}} . As this approximation has been found not to work very well for ultra-rare variants, a minimum minor allele count (MAC) is used to filter out these variants before testing (option --minMAC ).","title":"Saddle point approxiation (SPA) test"},{"location":"overview/#step-2-gene-based-testing","text":"Instead of performing single-variant association tests, multiple variants can be aggregated in a given region, such as a gene, using the following model g(\\mu) = w_1G_1\\beta_1 + \\dots + w_mG_m\\beta_m where G_i 's represent the single variants included in the test, w_i 's and \\beta_i 's are weights and effect sizes, respectively, for each variant, and g(.) is a link function for the phenotypic mean \\mu . We also denote by S_i the score statistics obtained from the single-variant tests . This can be especially helpful when testing rare variants as single-variant tests usually have lower power performance. To avoid inflation in the gene-based tests due to rare variants as well as reduce computation time, we have implemented the collapsing approach proposed in SAIGE-GENE+ 5 , where ultra-rare variants are aggregated into a mask. For highly imbalanced binary traits, SPA/Firth correction can be used to calibrate the test statistics in the gene-based tests as proposed in Zhao et al. (2020) 6 using --firth/--spa .","title":"Step 2 : Gene-based testing"},{"location":"overview/#burden-tests","text":"Burden tests, as defined in Lee et al. (2014) 7 , assume \\beta_i=\\beta\\; \\forall i , where \\beta is a fixed coefficient, which then leads to the test statistic Q_{BURDEN} = \\left(\\sum_i w_iS_i\\right)^2 These tests collapse variants into a single variable which is then tested for association with the phenotype. Hence, they are more powerful when variants have effects in the same direction and of similar magnitude. In regenie , multiple options are available to aggregate variants together into a burden mask beyond the linear combination above ( see here ). For example, the burden tests that were employed in Backman et al. (2021) 8 use the default strategy in regenie of collapsing variants by taking the maximum number of rare alleles across the sites.","title":"Burden tests"},{"location":"overview/#variance-component-tests","text":"Unlike burden tests, SKAT 9 assume the effect sizes $\\beta_i$ come from an arbitrary distribution with mean 0 and variance $\\tau^2$ which leads to the test statistic Q_{SKAT} = \\sum_i w_i^2S_i^2 Hence, SKAT can remain powerful when variant effects are in opposite directions. The omnibus test SKATO 10 combines the SKAT and burden tests as Q_{SKATO} = \\rho Q_{BURDEN} + (1-\\rho) Q_{SKAT} So setting $\\rho=0$ corresponds to SKAT and $\\rho=1$ to the burden test. In practice, the parameter $\\rho$ is chosen to maximize the power [ regenie uses a default grid of 8 values {$0, 0.1^2, 0.2^2, 0.3^2, 0.4^2, 0.5^2, 0.5, 1$} and set the weights $w_i = Beta(MAF_i,1,25)$]. To obtain the p-value from a linear combination of chi-squared variables, regenie uses Davies' exact method 11 by default. Following Wu et al (2016) 12 , regenie uses Kuonen's saddlepoint approximation method 13 when the Davies' p-value is below 1e-5 and if that fails, it uses Davies' method with more stringent convergence parameters (lim=1e5,acc=1e-9). The original SKATO method uses numerical integration when maximizing power across the various SKATO models that use different values for $\\rho$. We also implement a modification of SKATO, named SKATO-ACAT, which instead uses the Cauchy combination method 14 to combine the p-values from the different SKATO models.","title":"Variance component tests"},{"location":"overview/#cauchy-combination-tests","text":"The ACATV 15 test uses the Cauchy combination method to combine single variant p-values $p_i$ as Q_{ACATV} = \\sum_i \\widetilde{w}_i^2\\tan{\\{\\pi(0.5 - p_i)\\}} where we set $\\widetilde{w}_i = w_i \\sqrt{MAF(1-MAF)}$. This test is highly computationally tractable and is robust to correlation between the single variant tests. The omnibus test ACATO 15 combines ACATV with the SKAT and burden tests as Q_{ACATO} = \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{ACATV})\\}}+ \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{Burden})\\}}+ \\frac{1}{3}\\tan{\\{\\pi(0.5 - p_{SKAT})\\}} where unlike the original ACATO test, we only use one set of the weights $w_i$. Alternatively, we augment the test to include an extended set of SKATO models beyond SKAT and Burden (which correspond to $\\rho$ of 0 and 1 in SKATO respectively) and use the default SKATO grid of 8 values for $\\rho$.","title":"Cauchy combination tests"},{"location":"overview/#sparse-burden-association-test","text":"regenie can generate burden masks which are obtained by aggregating single variants using various annotation classes as well as allele frequency thresholds. The Sparse Burden Association Test (SBAT) 16 combines these burden masks in a joint model imposing constraints of same direction of effects \\mu = \\sum_{\\text{mask }i} M_i\\gamma_i where $M_i$ represent a burden mask and we solve \\underset{\\boldsymbol\\gamma}{\\min} || Y - \\sum_i M_i\\gamma_i||^2 \\text{ subject to } \\gamma_i \\ge 0 \\text{ for all } i The SBAT method tests the hypothesis $H_0: \\gamma_i=0$ for all $i$ vs. $H_1: \\gamma_i > 0$ for some $i$. By using this joint model, the SBAT test accounts for the correlation structure between the burden masks and with the non-negative constraints, it can lead to boost in power performance when multiple burden masks are causal and have concordant effects. This test has the nice property that it combines model selection of the masks (via the sparsity induced by the non-negative assumption) with model inference (it is well calibrated and powerful).","title":"Sparse Burden Association Test"},{"location":"overview/#step-2-interaction-testing","text":"The GxE tests are of the form g(\\mu) = E\\alpha + G\\beta + (G\\odot E)\\gamma where $E$ is an environmental risk factor and $G$ is a marker of interest, and $\\odot$ represents the Haddamard (entry-wise) product of the two. The last term in the model allows for the variant to have different effects across values of the risk factor. Note: if $E$ is categorical, we use a dummy variable for each level of $E$ in the model above. We can look at the following hypotheses: $H_0: \\beta = 0$ given $\\gamma = 0$, which is a marginal test for the SNP $H_0: \\beta = 0$, which is a test for the main effect of the SNP in the full model $H_0: \\gamma = 0$, which is a test for interaction $H_0: \\beta = \\gamma = 0$, which tests both main and interaction effects for the SNP Misspecification of the model above, such as in the presence of heteroskedasticity, or the presence of high case-control imbalance can lead to inflation in the tests. Robust (sandwich) standard error (SE) estimators 17 can be used to adress model misspecification however, they can suffer from inflation when testing rare variants or in the presence of high case-control imbalance 18 $^,$ 19 . In regenie , we use a hybrid approach which combines: Wald test with sandwich estimators Wald test with heteroskedastic linear models (for quantitative traits) LRT with penalized Firth regression (for binary traits) For quantitative traits, we use the sandwich estimators HC3 to perform a Wald test for variants whose minor allele count (MAC) is above 1000 (see --rare-mac ). For the remaining variants, we fit a heteroskedastic linear model (HLM) 20 Y = E\\alpha + E^2\\zeta + G\\beta + (G\\odot E)\\gamma + \\epsilon where we assume $\\epsilon \\sim N(0, D)$ where $D$ is a diagonal matrix with entries $\\sigma^2\\exp{(1 + E\\theta_1 + E^2\\theta_2)}$. This formulation allows for the phenotypic variance to also depend on the risk factor $E$. By incorporating both the linear and quadratic effect of $E$ in the mean and variance of $Y$, this model provides robustness to heteroskedasticity ( Note: the $E^2$ terms are only added when $E$ is quantitative ). Wald tests are then performed for the null hypotheses listed above. For binary traits, we consider the following interaction model \\text{logit}(\\mu) = E\\alpha + E^2\\zeta + G\\beta + (G\\odot E)\\gamma where we also include a non-linear effect for $E$ (not if categorical). The sandwich estimator HC3 is used in a Wald test for variants whose MAC is above 1000 (see --rare-mac ) otherwise the model-based standard errors are used. When Firth is specified, we only apply the Firth correction using LRT if the p-value for the interaction term $\\gamma$ from the Wald test is below a specified threshold (see --pThresh ). So the added $E^2$ term as well as the use of the Firth penalty help with case-control imbalance and model misspecification for the effect of $E$ on the phenotype.","title":"Step 2 : Interaction testing"},{"location":"overview/#missing-phenotype-data","text":"With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait. In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes from the analysis, use option --strict in step 1 and 2. This can also be used when analyzing a single trait to only keep individuals with complete data by setting the phenotype values of individuals to remove to NA. Note: imputation is only applied to phenotypes; covariates are not allowed to have missing data.","title":"Missing Phenotype data"},{"location":"overview/#references","text":"Breiman, L. Stacked regressions. Machine learning 24 , 49--64 (1996). \u21a9 Firth, D. Bias reduction of maximum likelihood estimates. Biometrika 80 , 27--38 (1993). \u21a9 Butler, R. W. Saddlepoint approximations with applications . (Cambridge University Press, 2007). \u21a9 Dey, R., Schmidt, E. M., Abecasis, G. R. & Lee, S. A fast and accurate algorithm to test for binary phenotypes and its application to PheWAS. The American Journal of Human Genetics 101 , 37--49 (2017). \u21a9 Zhou, W. et al. Set-based rare variant association tests for biobank scale sequencing data sets. medRxiv (2021). \u21a9 Zhao, Z. et al. UK biobank whole-exome sequence binary phenome analysis with robust region-based rare-variant test. Am J Hum Genet 106 , 3--12 (2020). \u21a9 Lee, S., Abecasis, G. R., Boehnke, M. & Lin, X. Rare-variant association analysis: Study designs and statistical tests. Am J Hum Genet 95 , 5--23 (2014). \u21a9 Backman, J. D. et al. Exome sequencing and analysis of 454,787 UK biobank participants. Nature 599 , 628--634 (2021). \u21a9 Wu, M. C. et al. Rare-variant association testing for sequencing data with the sequence kernel association test. Am J Hum Genet 89 , 82--93 (2011). \u21a9 Lee, S., Wu, M. C. & Lin, X. Optimal tests for rare variant effects in sequencing association studies. Biostatistics 13 , 762--75 (2012). \u21a9 Davies, R. B. Algorithm AS 155: The distribution of a linear combination of \u03c7 2 random variables. Applied Statistics 29 , 323--333 (1980). \u21a9 Wu, B., Guan, W. & Pankow, J. S. On efficient and accurate calculation of significance p-values for sequence kernel association testing of variant set. Ann Hum Genet 80 , 123--35 (2016). \u21a9 Kuonen, D. Miscellanea. Saddlepoint approximations for distributions of quadratic forms in normal variables. Biometrika 86 , 929--935 (1999). \u21a9 Liu, Y. & Xie, J. Cauchy combination test: A powerful test with analytic p-value calculation under arbitrary dependency structures. J Am Stat Assoc 115 , 393--402 (2020). \u21a9 Liu, Y. et al. ACAT: A fast and powerful p value combination method for rare-variant analysis in sequencing studies. Am J Hum Genet 104 , 410--421 (2019). \u21a9 \u21a9 Ziyatdinov, A., Barber, M. & Marchini, J. Pooling information across burden tests in the UK biobank exome sequencing study. ASHG Conference (2020). \u21a9 MacKinnon, J. G. & White, H. Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. Journal of Econometrics 29 , 305--325 (1985). \u21a9 Tchetgen Tchetgen, E. J. & Kraft, P. On the robustness of tests of genetic associations incorporating gene-environment interaction when the environmental exposure is misspecified. Epidemiology 22 , 257--61 (2011). \u21a9 Voorman, A., Lumley, T., McKnight, B. & Rice, K. Behavior of QQ-plots and genomic control in studies of gene-environment interaction. PLoS One 6 , (2011). \u21a9 Young, A. I., Wauthier, F. L. & Donnelly, P. Identifying loci affecting trait variability and detecting interactions in genome-wide association studies. Nat Genet 50 , 1608--1614 (2018). \u21a9","title":"References"},{"location":"performance/","text":"Performance We assessed the performance of regenie against 3 other programs for GWAS on large cohorts. BOLT-LMM Loh et al. (2015) Nature Genetics 47, 284\u2013290 [Software] SAIGE - Zhou et al. (2018) Nature Genetics 50, 1335\u20131341 [Software] fastGWA - Jiang et al. (2019) Nature Genetics 51, 1749\u20131755 [Software] Full details for all the runs are available in our paper . Quantitative traits We ran regenie , BOLT-LMM and fastGWA on 3 quantitative phenotypes measured on white British UK Biobank participants (LDL, N=389,189; Body mass index [BMI], N=407,609; and Bilirubin, N=388,303) where testing was performed on 9.8 million imputed SNPs. The Manhattan plots for all three phenotypes (see below) show good agreement between the methods with both regenie and BOLT-LMM resulting in stronger association signals relative to fastGWA at known peaks of association (note that in the plots, the scaling of the y-axis changes above the upper dashed line). We assessed the computational requirements of all three methods using a larger set of 50 quantitative traits from the UK Biobank, looking at computational timings as well as memory usage. For regenie and BOLT LMM, 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and for fastGWA, these SNPs were used to compute the sparse GRM (timing not included). Tests were performed on 11.4M imputed SNPs (step 2). From the table above, regenie was 151x faster than BOLT-LMM in elapsed time for Step 1 and 11.5x faster for Step 2, which translated into $>$30x overall speed-up in terms of elapsed time. In addition, regenie had a maximum memory usage of 12.9 GB, which is mostly due to regenie only reading a small portion of the genotype data at a time, whereas BOLT-LMM required 50GB. regenie was 2.8x faster than fastGWA, but fastGWA is very memory efficient and used only a maximum of 2GB. Binary traits regenie was compared to BOLT-LMM and SAIGE on a set of four binary traits measured on white British UK Biobank participants (coronary artery disease [CAD], N=352,063, case-control ratio=1:11; glaucoma, N=406,927, case-control ratio=1:52; colorectal cancer, N=407,746, case-control ratio=1:97; and thyroid cancer, N=407,746, case-control ratio=1:660) and Step 2 testing was performed on 11.6 million imputed SNPs. A novel and fast approximate Firth correction was used in regenie as well as a SPA correction. As seen in the Manhattan plots below (note that the scaling of the y-axis changes above the upper dashed line), all four approaches show very good agreement for the most balanced trait (CAD; case-control ratio=1:11), but as the fraction of cases decreases BOLT-LMM tends to give inflated test statistics. However both regenie with Firth and SPA corrections, as well as SAIGE, which uses SPA correction, are all robust to this inflation and show similar agreement for the associations detected. We assessed the computational requirements of regenie and SAIGE using a larger set of 50 binary traits from the UK Biobank that have a range of different case-control ratios and distinct missing data patterns. 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and tests were performed on 11.4M imputed SNPs (step 2). In step 1, regenie was run using LOOCV and for two traits SAIGE did not finish as it took longer than the 4-week limit. In step 2, the approximate Firth correction was used in regenie in addition to SPA correction. From the table above, Step 1 of regenie was about 350x faster and required only $40\\%$ of the memory used by SAIGE. In Step 2, regenie Firth and SPA were 2x and 3x faster than SAIGE in CPU time, respectively, but were 21x and 34x faster than SAIGE in elapsed time, respectively, which suggests that regenie makes better use of parallelization in this step. Overall, regenie using Firth correction was 8x faster than SAIGE in CPU hours and 26.8x faster in elapsed time. All runs above were done on the same computing environment (16 virtual CPU cores of a 2.1GHz AMD EPYC 7571 processor, 64GB of memory, and 600GB solid-state disk). Timings improvements in v2.2 We have several changes in regenie v2.2 to improve the computational efficiency: The genotype file reading in Step 1 is now multi-threaded for all supported formats (i.e. BED, PGEN, and BGEN) and uses a faster file reading implementation for BGEN v1.2 format with 8-bit encoding. From our timings experiments below, these changes helped reduce the CPU time by 40-60% depending on the input format. Note that we used a small number of SNPs for Step 1 in our experiments (20K) so the timing improvement will not be as high in a real Step 1 run where ~500K SNPs would be used. We have improved the implementation of the score tests for binary traits to reduce the number of matrix operations performed and this reduced the CPU timings by ~60% from the previous version 2.0.2. Note that there is an added memory cost of ~8NKP bytes [N=#samples; K=#covariates;P=#samples] so ~800MB extra for a UKB 500K run with 10 traits & 20 covariates. We have also made use of the sparsity of the genotype vector for rarer variants in Step 2 (more so with binary traits) and this reduced the timing in our experiments by ~20% on average. In our experiments, common variants are defined as having MAF > 5% and rare variants are defined as having MAF < 1% and no correction (i.e. Firth/SPA) is used. We have added new options --write-null-firth and --use-null-firth to reduce the timing of Step 2 with approximate Firth when ran in parallel jobs split in smaller chunks within chromosomes. More specifically, --write-null-firth can be used in Step 1 to fit the null model for approximate Firth test and store the resulting estimates to file. Then in Step 2, specifying --use-null-firth will re-use these parameter estimates to reduce the timing of the approximate Firth null model fitting. We thank Juha Karjalainen for suggesting this feature. Note: in our timings experiments, the PGEN genotype file only includes hard-calls. We ran a single trait in regenie and each setting was replicated 5 times. Gene-based testing regenie v3.0 adds in a wide range of gene-based tests . We have performed simulation experiments to assess the calibration of the tests with quantitative and binary traits using real genetic data from the UK Biobank where we randomly selected 100,000 samples obtained from the set of white British participants (see the \"Methods\" section of the Regenie paper for details on phenotype simulation where we set the heritability to 20%). Using whole exome sequencing data, we constructed variant sets incorporating functional annotations (LoF and missense, where missense vairants were predicted as deleterious using a score based on 5 in-silico algorithms), as well as allele frequency thresholds focusing on rarer variation (1%, 0.1% and 0.01%). The SKAT/ACAT tests were applied only to variant sets using a 1% or 0.01% AAF threshold and SBAT and BURDEN-ACAT joint tests combined all burden mask signals from the 1%, 0.1%, 0.01% and singleton thresholds. 1000 genes on even chromosomes were randonly selected and tested for association (causal variants were on odd chromosomes). The QQ plots below show the distribution p-values for each test across the different annotation categories (ran in Regenie v3.2). Quantitative traits Using a 1% allele frequency cutoff for the SKAT/ACAT tests. Binary traits We simulated highly imbalanced phenotypes with a disease prevalence of 1% (case-control ratio of 1:99) and applied Firth/SPA correction to the tests. Using a 1% allele frequency cutoff for the SKAT/ACAT tests. Using a 0.01% allele frequency cutoff for the SKAT/ACAT tests.","title":"Performance"},{"location":"performance/#performance","text":"We assessed the performance of regenie against 3 other programs for GWAS on large cohorts. BOLT-LMM Loh et al. (2015) Nature Genetics 47, 284\u2013290 [Software] SAIGE - Zhou et al. (2018) Nature Genetics 50, 1335\u20131341 [Software] fastGWA - Jiang et al. (2019) Nature Genetics 51, 1749\u20131755 [Software] Full details for all the runs are available in our paper .","title":"Performance"},{"location":"performance/#quantitative-traits","text":"We ran regenie , BOLT-LMM and fastGWA on 3 quantitative phenotypes measured on white British UK Biobank participants (LDL, N=389,189; Body mass index [BMI], N=407,609; and Bilirubin, N=388,303) where testing was performed on 9.8 million imputed SNPs. The Manhattan plots for all three phenotypes (see below) show good agreement between the methods with both regenie and BOLT-LMM resulting in stronger association signals relative to fastGWA at known peaks of association (note that in the plots, the scaling of the y-axis changes above the upper dashed line). We assessed the computational requirements of all three methods using a larger set of 50 quantitative traits from the UK Biobank, looking at computational timings as well as memory usage. For regenie and BOLT LMM, 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and for fastGWA, these SNPs were used to compute the sparse GRM (timing not included). Tests were performed on 11.4M imputed SNPs (step 2). From the table above, regenie was 151x faster than BOLT-LMM in elapsed time for Step 1 and 11.5x faster for Step 2, which translated into $>$30x overall speed-up in terms of elapsed time. In addition, regenie had a maximum memory usage of 12.9 GB, which is mostly due to regenie only reading a small portion of the genotype data at a time, whereas BOLT-LMM required 50GB. regenie was 2.8x faster than fastGWA, but fastGWA is very memory efficient and used only a maximum of 2GB.","title":"Quantitative traits"},{"location":"performance/#binary-traits","text":"regenie was compared to BOLT-LMM and SAIGE on a set of four binary traits measured on white British UK Biobank participants (coronary artery disease [CAD], N=352,063, case-control ratio=1:11; glaucoma, N=406,927, case-control ratio=1:52; colorectal cancer, N=407,746, case-control ratio=1:97; and thyroid cancer, N=407,746, case-control ratio=1:660) and Step 2 testing was performed on 11.6 million imputed SNPs. A novel and fast approximate Firth correction was used in regenie as well as a SPA correction. As seen in the Manhattan plots below (note that the scaling of the y-axis changes above the upper dashed line), all four approaches show very good agreement for the most balanced trait (CAD; case-control ratio=1:11), but as the fraction of cases decreases BOLT-LMM tends to give inflated test statistics. However both regenie with Firth and SPA corrections, as well as SAIGE, which uses SPA correction, are all robust to this inflation and show similar agreement for the associations detected. We assessed the computational requirements of regenie and SAIGE using a larger set of 50 binary traits from the UK Biobank that have a range of different case-control ratios and distinct missing data patterns. 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and tests were performed on 11.4M imputed SNPs (step 2). In step 1, regenie was run using LOOCV and for two traits SAIGE did not finish as it took longer than the 4-week limit. In step 2, the approximate Firth correction was used in regenie in addition to SPA correction. From the table above, Step 1 of regenie was about 350x faster and required only $40\\%$ of the memory used by SAIGE. In Step 2, regenie Firth and SPA were 2x and 3x faster than SAIGE in CPU time, respectively, but were 21x and 34x faster than SAIGE in elapsed time, respectively, which suggests that regenie makes better use of parallelization in this step. Overall, regenie using Firth correction was 8x faster than SAIGE in CPU hours and 26.8x faster in elapsed time. All runs above were done on the same computing environment (16 virtual CPU cores of a 2.1GHz AMD EPYC 7571 processor, 64GB of memory, and 600GB solid-state disk).","title":"Binary traits"},{"location":"performance/#timings-improvements-in-v22","text":"We have several changes in regenie v2.2 to improve the computational efficiency: The genotype file reading in Step 1 is now multi-threaded for all supported formats (i.e. BED, PGEN, and BGEN) and uses a faster file reading implementation for BGEN v1.2 format with 8-bit encoding. From our timings experiments below, these changes helped reduce the CPU time by 40-60% depending on the input format. Note that we used a small number of SNPs for Step 1 in our experiments (20K) so the timing improvement will not be as high in a real Step 1 run where ~500K SNPs would be used. We have improved the implementation of the score tests for binary traits to reduce the number of matrix operations performed and this reduced the CPU timings by ~60% from the previous version 2.0.2. Note that there is an added memory cost of ~8NKP bytes [N=#samples; K=#covariates;P=#samples] so ~800MB extra for a UKB 500K run with 10 traits & 20 covariates. We have also made use of the sparsity of the genotype vector for rarer variants in Step 2 (more so with binary traits) and this reduced the timing in our experiments by ~20% on average. In our experiments, common variants are defined as having MAF > 5% and rare variants are defined as having MAF < 1% and no correction (i.e. Firth/SPA) is used. We have added new options --write-null-firth and --use-null-firth to reduce the timing of Step 2 with approximate Firth when ran in parallel jobs split in smaller chunks within chromosomes. More specifically, --write-null-firth can be used in Step 1 to fit the null model for approximate Firth test and store the resulting estimates to file. Then in Step 2, specifying --use-null-firth will re-use these parameter estimates to reduce the timing of the approximate Firth null model fitting. We thank Juha Karjalainen for suggesting this feature. Note: in our timings experiments, the PGEN genotype file only includes hard-calls. We ran a single trait in regenie and each setting was replicated 5 times.","title":"Timings improvements in v2.2"},{"location":"performance/#gene-based-testing","text":"regenie v3.0 adds in a wide range of gene-based tests . We have performed simulation experiments to assess the calibration of the tests with quantitative and binary traits using real genetic data from the UK Biobank where we randomly selected 100,000 samples obtained from the set of white British participants (see the \"Methods\" section of the Regenie paper for details on phenotype simulation where we set the heritability to 20%). Using whole exome sequencing data, we constructed variant sets incorporating functional annotations (LoF and missense, where missense vairants were predicted as deleterious using a score based on 5 in-silico algorithms), as well as allele frequency thresholds focusing on rarer variation (1%, 0.1% and 0.01%). The SKAT/ACAT tests were applied only to variant sets using a 1% or 0.01% AAF threshold and SBAT and BURDEN-ACAT joint tests combined all burden mask signals from the 1%, 0.1%, 0.01% and singleton thresholds. 1000 genes on even chromosomes were randonly selected and tested for association (causal variants were on odd chromosomes). The QQ plots below show the distribution p-values for each test across the different annotation categories (ran in Regenie v3.2).","title":"Gene-based testing"},{"location":"performance/#quantitative-traits_1","text":"Using a 1% allele frequency cutoff for the SKAT/ACAT tests.","title":"Quantitative traits"},{"location":"performance/#binary-traits_1","text":"We simulated highly imbalanced phenotypes with a disease prevalence of 1% (case-control ratio of 1:99) and applied Firth/SPA correction to the tests. Using a 1% allele frequency cutoff for the SKAT/ACAT tests. Using a 0.01% allele frequency cutoff for the SKAT/ACAT tests.","title":"Binary traits"},{"location":"recommendations/","text":"Recommendations for UK Biobank analysis regenie is ideally suited for large-scale analyses such as 500K UK Biobank (UKBB) data, where records are available for thousands of phenotypes. We provide below a few guidelines on how to perform such analysis on the UKBB files that all UKBB approved researchers have access to. Pre-processing We will first go over important steps to consider before running regenie . Selection of traits regenie can perform whole genome regression on multiple traits at once, which is where higher computational gains are obtained. As different traits can have distinct missing patterns, regenie uses an imputation scheme to handle missing data. From the real data applications we have performed so far with traits having up to ~20% (for quantitative) and ~5% (for binary) missing observations, our imputation scheme resulted in nearly identical results as from discarding missing observations when analyzing each trait separately (see the paper for details). Hence, we recommend to analyze traits in groups that have similar missingness patterns with resonably low amount of missingness (<15%). The number of phenotypes in a group will affect the computational resources required and the table below shows typical computational requirements based on using 500,000 markers in step 1 split in blocks of 1000 and using blocks of size 200 when testing SNPs in step 2. The estimates are shown when step 1 of regenie is run in low-memory mode so that within-block predictions are temporarily stored on disk (see Documentation). In the following sections, we'll assume traits (let's say binary) and covariates used in the analysis have been chosen and data are in files ukb_phenotypes_BT.txt and ukb_covariates.txt , which follow the format requirement for regenie (see Documentation). Preparing genotype file Step 1 of a regenie run requires a single genotype file as input; we recommend using array genotypes for this step. The UKBB genotype files are split by chromosome, so we recommend using PLINK to merge the files using the following code. NOTE : please change XXX to you own UKBB application ID number rm -f list_beds.txt for chr in {2..22}; do echo \"ukb_cal_chr${chr}_v2.bed ukb_snp_chr${chr}_v2.bim ukbXXX_int_chr1_v2_s488373.fam\" >> list_beds.txt; done plink \\ --bed ukb_cal_chr1_v2.bed \\ --bim ukb_snp_chr1_v2.bim \\ --fam ukbXXX_int_chr1_v2_s488373.fam \\ --merge-list list_beds.txt \\ --make-bed --out ukb_cal_allChrs Exclusion files Quality control (QC) filters can be applied using PLINK2 to filter out samples and markers in the genotype file prior to step 1 of regenie . Note: regenie will throw an error if a low-variance SNP is included in the step 1 run. Hence, the user should run adequate QC filtering prior to running regenie to identify and remove such SNPs. For example, to filter out SNPs with minor allele frequency (MAF) below 1%, minor allele count (MAC) below 100, genotype missingess above 10% and Hardy-Weinberg equilibrium p-value exceeding 10^{-15} , and samples with more than 10% missingness, plink2 \\ --bfile ukb_cal_allChrs \\ --maf 0.01 --mac 100 --geno 0.1 --hwe 1e-15 \\ --mind 0.1 \\ --write-snplist --write-samples --no-id-header \\ --out qc_pass Step 1 We recommend to run regenie using multi-threading (8+ threads) which will decrease the overall runtime of the program. As this step can be quite memory intensive (due to storing block predictions), we recommend to use option --lowmem , where the number of phenotypes analyzed will determine how much disk space is required (see table above). Running step 1 of regenie (by default, all available threads are used) ./regenie \\ --step 1 \\ --bed ukb_cal_allChrs \\ --extract qc_pass.snplist \\ --keep qc_pass.id \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --bsize 1000 \\ --lowmem \\ --lowmem-prefix tmpdir/regenie_tmp_preds \\ --out ukb_step1_BT For P phenotypes analyzed, this will generate a set of $P$ files ending with .loco which contain the genetic predictions using a LOCO scheme that will be needed for step 2, as well as a prediction list file ukb_step1_BT_pred.list , which lists the names of these predictions files and can be used as input for step 2. Step 2 As step 1 and 2 are completely decoupled in regenie , you could either use all the traits for testing in step 2 or select a subset of the traits to perform association testing. Furthermore, you can use the same Step 1 output to test on array, exome or imputed variants; below, we will illustrate testing on imputed variants. Step 2 of regenie can be run in parallel across chromosomes so if you have access to multiple machines, we recommend to split the runs over chromosomes (using 8+ threads). Running regenie tesing on a single chromosome (here chromosome 1) and using the fast Firth correction as fallback for p-values below 0.01 ./regenie \\ --step 2 \\ --bgen ukb_imp_chr1_v3.bgen \\ --ref-first \\ --sample ukbXXX_imp_chr1_v3_s487395.sample \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --firth --approx --pThresh 0.01 \\ --pred ukb_step1_BT_pred.list \\ --bsize 400 \\ --split \\ --out ukb_step2_BT_chr1 This will create separate association results files for each phenotype as ukb_step2_BT_chr1_*.regenie . When running the SKAT/ACAT gene-based tests, we recommend to use at most 2 threads and instead parallelize the runs over partitions of the genome (e.g. groups of genes).","title":"UKBB Analysis"},{"location":"recommendations/#recommendations-for-uk-biobank-analysis","text":"regenie is ideally suited for large-scale analyses such as 500K UK Biobank (UKBB) data, where records are available for thousands of phenotypes. We provide below a few guidelines on how to perform such analysis on the UKBB files that all UKBB approved researchers have access to.","title":"Recommendations for UK Biobank analysis"},{"location":"recommendations/#pre-processing","text":"We will first go over important steps to consider before running regenie .","title":"Pre-processing"},{"location":"recommendations/#selection-of-traits","text":"regenie can perform whole genome regression on multiple traits at once, which is where higher computational gains are obtained. As different traits can have distinct missing patterns, regenie uses an imputation scheme to handle missing data. From the real data applications we have performed so far with traits having up to ~20% (for quantitative) and ~5% (for binary) missing observations, our imputation scheme resulted in nearly identical results as from discarding missing observations when analyzing each trait separately (see the paper for details). Hence, we recommend to analyze traits in groups that have similar missingness patterns with resonably low amount of missingness (<15%). The number of phenotypes in a group will affect the computational resources required and the table below shows typical computational requirements based on using 500,000 markers in step 1 split in blocks of 1000 and using blocks of size 200 when testing SNPs in step 2. The estimates are shown when step 1 of regenie is run in low-memory mode so that within-block predictions are temporarily stored on disk (see Documentation). In the following sections, we'll assume traits (let's say binary) and covariates used in the analysis have been chosen and data are in files ukb_phenotypes_BT.txt and ukb_covariates.txt , which follow the format requirement for regenie (see Documentation).","title":"Selection of traits"},{"location":"recommendations/#preparing-genotype-file","text":"Step 1 of a regenie run requires a single genotype file as input; we recommend using array genotypes for this step. The UKBB genotype files are split by chromosome, so we recommend using PLINK to merge the files using the following code. NOTE : please change XXX to you own UKBB application ID number rm -f list_beds.txt for chr in {2..22}; do echo \"ukb_cal_chr${chr}_v2.bed ukb_snp_chr${chr}_v2.bim ukbXXX_int_chr1_v2_s488373.fam\" >> list_beds.txt; done plink \\ --bed ukb_cal_chr1_v2.bed \\ --bim ukb_snp_chr1_v2.bim \\ --fam ukbXXX_int_chr1_v2_s488373.fam \\ --merge-list list_beds.txt \\ --make-bed --out ukb_cal_allChrs","title":"Preparing genotype file"},{"location":"recommendations/#exclusion-files","text":"Quality control (QC) filters can be applied using PLINK2 to filter out samples and markers in the genotype file prior to step 1 of regenie . Note: regenie will throw an error if a low-variance SNP is included in the step 1 run. Hence, the user should run adequate QC filtering prior to running regenie to identify and remove such SNPs. For example, to filter out SNPs with minor allele frequency (MAF) below 1%, minor allele count (MAC) below 100, genotype missingess above 10% and Hardy-Weinberg equilibrium p-value exceeding 10^{-15} , and samples with more than 10% missingness, plink2 \\ --bfile ukb_cal_allChrs \\ --maf 0.01 --mac 100 --geno 0.1 --hwe 1e-15 \\ --mind 0.1 \\ --write-snplist --write-samples --no-id-header \\ --out qc_pass","title":"Exclusion files"},{"location":"recommendations/#step-1","text":"We recommend to run regenie using multi-threading (8+ threads) which will decrease the overall runtime of the program. As this step can be quite memory intensive (due to storing block predictions), we recommend to use option --lowmem , where the number of phenotypes analyzed will determine how much disk space is required (see table above). Running step 1 of regenie (by default, all available threads are used) ./regenie \\ --step 1 \\ --bed ukb_cal_allChrs \\ --extract qc_pass.snplist \\ --keep qc_pass.id \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --bsize 1000 \\ --lowmem \\ --lowmem-prefix tmpdir/regenie_tmp_preds \\ --out ukb_step1_BT For P phenotypes analyzed, this will generate a set of $P$ files ending with .loco which contain the genetic predictions using a LOCO scheme that will be needed for step 2, as well as a prediction list file ukb_step1_BT_pred.list , which lists the names of these predictions files and can be used as input for step 2.","title":"Step 1"},{"location":"recommendations/#step-2","text":"As step 1 and 2 are completely decoupled in regenie , you could either use all the traits for testing in step 2 or select a subset of the traits to perform association testing. Furthermore, you can use the same Step 1 output to test on array, exome or imputed variants; below, we will illustrate testing on imputed variants. Step 2 of regenie can be run in parallel across chromosomes so if you have access to multiple machines, we recommend to split the runs over chromosomes (using 8+ threads). Running regenie tesing on a single chromosome (here chromosome 1) and using the fast Firth correction as fallback for p-values below 0.01 ./regenie \\ --step 2 \\ --bgen ukb_imp_chr1_v3.bgen \\ --ref-first \\ --sample ukbXXX_imp_chr1_v3_s487395.sample \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --firth --approx --pThresh 0.01 \\ --pred ukb_step1_BT_pred.list \\ --bsize 400 \\ --split \\ --out ukb_step2_BT_chr1 This will create separate association results files for each phenotype as ukb_step2_BT_chr1_*.regenie . When running the SKAT/ACAT gene-based tests, we recommend to use at most 2 threads and instead parallelize the runs over partitions of the genome (e.g. groups of genes).","title":"Step 2"}]}